= Einleitung =

Dieser Artikel soll in mehreren Etappen an das Thema Spieleentwicklung heranführen. Eine allgemeine Einführung in die Thematik bildet dabei den Grundstock. Diese soll Grundbegriffe und Mechanismen überlicksartig erklären. Anschließend werden die einzelnen Teilaufgaben bei der Entwicklung eines Spiels erklärt. Die so erarbeiteten Themen werden abschließend in eine kleine Space Invaders Variante gegossen.

Als kleine Vorwarnung: Ich schreibe seit ca. 10 Jahren kleinere und größere Spiele. Meine Herangehensweise ist sicher nicht die optimalste. Auch können mir im Rahmen dieses Artikels faktische Fehler unterlaufen, ich werde aber versuchen diese zu minimieren. Auch werde ich wo angebracht Anglizismen verwenden da diese das googlen nach weiterführendem Material erleichtern. Auch werde ich einige Wikipedia Links einbauen für Begriffe die man eventuel Nachschlagen möchte. Los geht's. 

= Was muss ich vorab wissen? =
In diesem Artikel setze ich ein paar Dinge als Minimum vorraus. Keine Angst, höhere Mathematik gehört nicht dazu :)

* [http://www.mindview.net/Books/TIJ/ Java]
* Handhabung von [http://www.eclipse.org/ Eclipse]
* Installiertes [http://developer.android.com/sdk/index.html Android SDK] sowie [http://developer.android.com/sdk/eclipse-adt.html Eclipse Plugin]
* [http://developer.android.com/reference/android/app/Activity.html#ActivityLifecycle Activity Life Cycle]
* [http://developer.android.com/guide/developing/eclipse-adt.html Erstellen eines neuen Android Projekts in Eclipse]

Den Code zu diesem Tutorial könnt ihr euch per SVN von der Addresse http://android-gamedev.googlecode.com/svn/trunk/ holen. Im Projekt gibt es eine Main Activity die euch gleich wie in den SDK Demos eine Liste an Applikationen zeigt. Einfach das Projekt aus dem SVN auschecken, in Eclipse importieren und eine Run Configuration anlegen und die Default Launch Activity starten. 

= Wo fang ich an? =
Am Anfang jedes Spiels steht eine Idee. Wird es ein Puzzler? Ein Rundenstrategiespiel? Ein First-Person-Shooter? Auch wenn diese Genres unterschiedlicher nicht sein könnten, so unterscheiden sie sich im Grunde ihres Daseins oft wenig. Ein Spiel kann in mehrere  Module zur Erledigung diverser Aufgaben eingeteilt werden:

* '''Applikationsgerüst'''
* '''Eingabe Modul'''
* '''Datei I/O Modul'''
* '''Grafik Modul'''
* '''Sound Modul'''
* '''Netzwerk Modul'''
* '''Simulations Modul'''

Im folgenden wollen wir uns mit diesen 6 Modulen etwas genauer beschäftigen.

== Applikationsgerüst ==
Das Applikationsgerüst stellt die Basis für das Spiel dar. In der Regel ähnelt dieses herkömmlichen Applikationsgerüsten nicht. Spiele sind in den meisten Fällen nicht Event-basiert, d.h. sie laufen ständig, zeichnen dabei die Spielwelt permanent neu, holen sich dauernd neuen User Input und simulieren die Welt (für Kartenspiele und Ähnliches muss dies natürlich nicht gelten). So man nicht gerade für eine Konsole programmiert, stellt sich einem jedoch das Problem, dass die meisten Betriebssystem Event-basierte Programmierung als Paradigma gewählt haben. So auch auf Android. Applikationen werden dabei nur bei Bedarf neu gezeichnet, zum Beispiel wenn der User Text eingibt, einen Button drückt und so weiter, beziehungsweise wird Code nur dann ausgeführt wenn es eine User-Eingabe gibt. Im Allgemeinen hebelt man dies aus, indem man einen seperaten Thread startet der das Betriebssystem veranlasst die Applikation permanent neu zu zeichnen. In diesem Thread befindet sich so gut wie immer eine Schleife, auch Main Loop genannt, innerhalb derer sich immer das selbe abspielt (stark vereinfacht):

 while( !done )
 {
    processInput( )
    simulateWorld( )
    renderWorld( )   
 }  

Wie genau dieser Main Loop ausschaut hängt von vielerlei Faktoren ab, zum Beispiel dem verwendeten Betriebssystem, dem Spiel selbst und so weiter. 

Im Rahmen dieses Artikels werden wir sehen wir man dieses Konzept äußerst einfach auf Android implementieren kann. 

== Eingabe Modul ==
Um dem Spieler die Möglichkeit zu geben in das Spielsystem einzugreifen müssen dessen Eingaben irgendwie gelesen werden. Diese Aufgabe übernimmt das Eingabe Modul. Tastaturen, Mäuse, Touch-Screens, Joysticks, Gamepads und einige andere exotische Möglichkeiten stehen hier zur Verfügung. Wie man an die Eingabe kommt ist dabei wieder Betriebssystem-abhängig. 

Android verfügt über einige Eingabe Möglichkeiten. Wir werden uns mit den wichtigsten zwei beschäftigen: dem Touch-Screen sowie dem Accelerometer.

== Datei I/O Modul ==
Alle Resourcen eines Spieles müssen in irgendeiner Form der Applikation zugänglich gemacht werden. In speziellen Fällen kann das Spiel diese On-the-fly zur Laufzeit prozedural selbst erstellen, meistens liegen diese aber in Form von Dateien auf einem Datenträger vor. Auch hier gibt es verschiedene Möglichkeiten der Ablage: Dateien können schön geordnet in Ordnern abgelegt werden, wild in einem einzigen Ordner gespeichert sein oder gar in einem Zip-File fein säuberlich gepackt vorliegen. Das Datei I/O Modul soll dies abstrahieren damit im Programmcode der Zugriff auf die Resourcen erleichtert wird. 

Android bietet hier schon einen netten Ansatz mit seinem Resourcen und Asset System auf das wir später noch zu sprechen kommen werden.

== Grafik Modul ==
In unseren modernen Zeiten ist für viele Spieleentwickler dieser Teil eines Spiels wohl der wichtigste (teils zu Lasten des Spielspaßes). Dieses Modul übernimmt die Darstellung sämtlicher grafischen Inhalte des Spieles, sei dies das User Interface, welches in der Regel zweidimensional ist oder die Spielewelt selbst, meist in der dritten Dimension. Da letzteres oft rechenintensiv ist, wird spezielle Hardware verwendet um das ganze zu beschleunigen. Die Kommunikation mit dieser Hardware, ihr klar zu machen wie wo was gezeichnet werden soll, sowie die Verwaltung von grafischen Resourcen wie [http://en.wikipedia.org/wiki/Bitmap Bitmaps] und Geometry (auch [http://en.wikipedia.org/wiki/Polygon_mesh Meshes] genannt) ist die Hauptaufgabe dieses System. Hierunter fallen auch Dinge wie das Zeichnen von [http://en.wikipedia.org/wiki/Particle_system Partikel-Effekten] oder der Einsatz von sogenannten [http://en.wikipedia.org/wiki/Shader Shadern] (auf Android noch nicht möglich). Ganz allgemein kann festgehalten werden das die meisten Objekte die simuliert werden auch eine grafische Entsprechung haben. Meiner Erfahrung nach ist es äußerst hilfreich die simulierte Welt komplett unabhängig von der grafischen Darstellung zu machen. Das Grafikmodul holt sich lediglich Information von der Welt-Simulation, hat aber auf diese keinen Einfluss. Wem das etwas zu wage ist: keine Angst der Zusammenhang sollte spätestens beim Entwickeln des Space Invader Clones erkenntlich werden. Es sei jedoch gesagt, dass dieser Ansatz es erlaubt das Grafik Modul beliebig aus zu tauschen, zum Beispiel statt einer 2D Darstellung das ganze auf 3D zu portieren, ohne dass dabei der Simulationsteil geändert werden müsste.

Aufmerksamen Lesern ist vielleicht der Zusammenhang zwischen dem Grafik Modul und dem Main Loop bereits aufgefallen. Neuste Grafikkarten wird meist mit Benchmarks zu Leibe gerückt die die sogenannten Frames per Second (kurz FPS) oder [http://en.wikipedia.org/wiki/Frames_per_second#Frame_rates_in_video_games Frame Rate] messen. Diese geben an wie oft der Main Loop in einer Sekunde durchlaufen wurde. Es wird also gezählt wie oft der Main Loop (Input verarbeiten, Welt simulieren und das ganze dann zu zeichnen) in einer Sekunde durchlaufen wird. Im Zuge unserer Unternehmung werden wir immer ein Auge auf die Frame Rate werfen um etwaige Bottlenecks in unserem Spiel identifizieren zu können. 

Später werden wir sehen wie wir Android 2D und 3D Grafiken über [http://en.wikipedia.org/wiki/OpenGL OpenGL ES] entlocken können.

== Sound Modul ==
Soundeffekte und Musik gehören zu jedem guten Spiel. Dementsprechend kümmert sich das Sound Modul um das Abspielen solcher Resourcen. Dabei gibt es zwischen Soundeffekten und Musik einen wichtigen Unterschied: Soundeffekte sind in der Regel sehr klein (Kilobyte Bereich) und werden direkt im Hauptspeicher gehalten da sie oft verwendet werden, zum Beispiel das Feuergeräusch einer Kannone. Musik wiederum liegt oft in komprimierter Form vor (mp3, ogg) und braucht unkomprimiert (und damit abspielbar) massig Speicher. Sie wird daher meist gestreamed, das heißt bei Bedarf stückweise von der Festplatte oder einem anderen Medium (DVD,Internet) nachgeladen. Auf Implementationsebene macht dies oft einen Unterschied da dieser Nachlademechanismus meist selbst implementiert werden muss. 

In Zeiten von Surround Sound Heimsystemen legen Spieleentwickler auch wert auf dreidimensionalen Klang, meist gleich wie bei der Grafik Hardware-beschleunigt. Android bietet diese Möglichkeit noch nicht erlaubt aber relativ schmerzlos das Abspielen von Soundeffekten und das streamen von Musik wie wir später noch sehen werden.

== Netzwerk Modul ==
Seit World of Warcraft und Counter Strike ist klar: an Multiplayer-Möglichkeiten kommt kein modernes Spiel vorbei. Das Netzwerk Modul hat dabei gleich mehrere Aufgaben zu stemmen. Auf der einen Seite handhabt es die Kommunikation mit etwaigen Servern, sendet Mitteilungen von Spielern herum, lädt von Spielern gebastelte Levels ins Netz und so weiter. Dies sind quasi administrative Aufgaben und haben nur indirekt mit dem Spielgeschehen selbst zu tun. Auf der anderen Seite gilt es Spieledaten wie aktuelle Positionen, das Abfeuern von Kugeln, das entsenden von Truppen und vieles mehr den anderen an der Partie teilhabenden Rechnern im Netz mit zu teilen. Abhängig vom Genre des Spiels kommen hier verschiedene Methoden zum Einsatz um das Spielgeschehen zu synchronisieren. Dieser Themenbereich ist so groß und komplex das ihm am besten ein eigener Artikel gewidmet werden sollte. Im Rahmen dieses Textes werde ich nicht weiter auf diese Komponente eingehen. 

== Simulations Modul ==
Damit sich die Dinge im Spiel bewegen muss man sie auch irgendwie antreiben. Das ist die Aufgabe des Simulationsmoduls. Es beinhaltet sämtliche Informationen zum Spielgeschehen selbst wie die Position von Spielfiguren, deren aktuelle Aktion, wieviel Munition noch über ist und so weiter. Auf Basis der User Eingaben sowie der Entscheidungen einer etwaig implementierten Künstlichen Intelligenz wird das Verhalten der Spielobjekte simuliert. Die Simulation läuft dabei immer Schrittweise ab. In jedem Durchgang des Main Loops wird die Simulation um einen Schritt vorangetrieben. Dies passiert zumeist Zeit-basiert, das heißt man simuliert eine bestimmte Zeitspanne. Für ein weiches Ablaufen wird als Zeitspanne meist die Zeit die seit dem zeichnen des letzten Frames vergangen ist herangezogen. Als kleines Beispiel: gegeben einer Kanonenkugel die mit 10m/s nach rechts fliegt schreiten wir einen Schritt in der Simulation weiter. Die Zeitspanne seit dem letzten Frame beträgt 0.016s (16 Millisekunden, entspricht einer Frame Rate von 60fps). 10ms/s * 0.016s = 0.16m, das heißt die Kannonenkugel ist nach Abschluss dieses Simulationsschrittes um 16 Zentimeter weiter links im Vergleich zum letzten Frame. Diese Art des Simulationsschrittes nennt man [http://www.gamedev.net/reference/articles/article1604.asp Frame Independent Movement] und sollte Bestandteil jedes Simulations Modules sein. Wie der Name schon sagt ist es egal wieviel FPS das Spiel schafft, die Kanonenkugel wird sich auf allen Systemen gleich verhalten (wenn auch die Zwischenschritte andere sein mögen). Es sei angemerkt dass bei Verwendung von Physik Systemen man meist fixe Zeitschritte verwendet da die kleinen Schwankungen beim Messen der Zeitspanne zwischen dem aktuellen und dem letzten Frame viele Physik Systeme instabil machen können. Wir werden in unserem Space Invaders Clone keine grandiosen Physikspielereien implementieren, daher bleiben wir bei der herkömmlichen Zeit-basierten Methode die die Frame Zeitspanne heranzieht.

Abhängig vom Spieletyp gehört auch die künstliche Intelligenz zum Simulations Modul. Diese kann sehr simpel Ausfallen, zum Beispiel das Verhalten der Goombas in Super Mario die nur dumm nach rechts und links laufen. In Echtzeitstrategiespielen kann diese schon um einiges komplexer werden. Der Terminus künstliche Intelligenz ist hier streng gesehen auch nicht ganz korrekt, in Ermangelung eines besseren Begriffs bleiben wir aber einfach dabei. 

= Und auf Android? =
Wir wollen nur für all die oben genannten Module eine Entsprechung auf Android entwickeln. Wir beginnen mit der Activity selbst und versuchen das Main Loop Muster dort zu implementieren. Das Managen von Dateien über Resourcen und Assets werden wir uns als nächster anschaun. Anschließend werden wir uns näher mit OpenGL ES beschäftigen und Android zwingen für uns interessante Dinge zu zeichnen. Die Ausgabe von Soundeffekten und Musik bildet den Abschluss dieses Kapitels womit wir dann für unseren Space Invaders Clone gerüstet sind. 

Im Zuge dieses Kapitels werden wir wiederverwendbare Komponenten bauen. Schließlich wäre es nicht sinnvoll jedes mal das Rad neu zu erfinden. Alle Codes könnt ihr unter [http://code.google.com/p/android-gamedev/ http://code.google.com/p/android-gamedev/] finden und per SVN auschecken. Das Projekt beinhaltet ein paar Beispielprogramme zu den einzelnen Abschnitten sowie den Space Invaders Clone selbst.

== Android Activity ==
Das Grundgerüst unseres Spiels bildet eine simple Activity. Dabei ergibt sich ein klassisches Henne/Ei Problem: wir müssen hier schon mit OpenGL ES beginnen ohne uns schon damit aus zu kennen. Aber keine Angst, das ganze erweist sich als relativ einfach. 

Ziel in diesem Kapitel wird es sein eine Lauffähige OpenGL ES Activity zu bauen, die den grundlegenden Activity Life Cycle respektiert. Seit SDK 1.5 gibt es die sogenannten [http://developer.android.com/reference/android/opengl/GLSurfaceView.html GLSurfaceView]. Sie ist ein GUI Baustein ähnlich zum Beispiel einer List View die man einfach in die Activity einhängt und die das initialisieren von OpenGL mit halbwegs guten Parametern für uns übernimmt. Des weiteren startet sie einen zweiten Thread neben dem GUI Thread der Activity der das Neuzeichnen des Geschehens permanent anstößt. Hier sieht man schon erste paralleln zum Main Loop Konzept. Wir werden dies ausnützen.

Damit wir eine Möglichkeit haben das Neuzeichnen selbst zu übernehmen bietet die GLSurfaceView ein Listener Konzept (auch [http://en.wikipedia.org/wiki/Observer_pattern Observer Design Pattern] genannt). Eine Applikation die sich in den Rendering-Thread der GLSurfaceView einhängen möchte registriert bei dieser eine Implementierung des Interface [http://developer.android.com/reference/android/opengl/GLSurfaceView.Renderer.html Renderer]. Dieses Interface hat drei Methoden die abhängig vom Status der GLSurfaceView aufgerufen werden:

 public abstract void onDrawFrame(GL10 gl)
 public abstract void onSurfaceCreated(GL10 gl, EGLConfig config)
 public abstract void onSurfaceChanged(GL10 gl, int width, int height)
 
Die Methode [http://developer.android.com/reference/android/opengl/GLSurfaceView.Renderer.html#onDrawFrame(javax.microedition.khronos.opengles.GL10) onDrawFrame] ist jene die die GLSurfaceView jedesmal beim Neuzeichnen aufruft. Den Parameter ''gl'' den wir dabei erhalten werden wir später genauer Besprechen. 

Die Methode [http://developer.android.com/reference/android/opengl/GLSurfaceView.Renderer.html#onSurfaceCreated(javax.microedition.khronos.opengles.GL10,%20javax.microedition.khronos.egl.EGLConfig) onSurfaceCreated] wird aufgerufen sobald die GLSurfaceView das fertig initialisiert ist. Hier kann man verschiedene Setup Aufgaben erledigen wie zum Beispiel das laden von Resourcen. 

Die Methode [http://developer.android.com/reference/android/opengl/GLSurfaceView.Renderer.html#onSurfaceChanged(javax.microedition.khronos.opengles.GL10,%20int,%20int) onSurfaceChanged] wird aufgerufen wenn sich die Abmessungen der GLSurfaceView ändern. Dies passiert wenn der User das Android Device kippt und so in den Portrait oder Landscape Modus schaltet. Die Parameter ''width'' und ''height'' geben uns dabei die Breite und Höhe des Bereiches an auf den wir zeichnen und zwar in Pixel. Diese Information werden wir später noch benötigen. 

Unsere erste Activity hat also ein paar Aufgaben:

* Erstellen einer GLSurfaceView und Einhängen in die Activity
* Setzen einer Renderer Implementierung für die GLSurfaceView
* Implementierung der Rendererer Implementierung

Für eine saubere Implementierung werden wir einfach die Klasse Activity ableiten und diese ''GameActivity'' nennen. Dieser verpassen wir einen Member vom Typ GLSurfaceView den wir in der ''onCreate'' Methode der Klasse instanzieren und in die Activity einhängen. Weiters implementiert unsere abgeleitete Activity das Interface Renderer. In zweit weiteren Member Variablen speichern wird die aktuelle Größe des zu bemalenden Bereichs die wir beim Aufruf der Methode onSurfaceCreated in Erfahrung bringen. Diesen Bereich nennt man im übrigen auch [http://en.wikipedia.org/wiki/Viewport Viewport]. Wir werden diese Terminologie fortan übernehmen. Wir verpassen der Klasse noch Getter Methoden damit wir später auf die Abmessungen zugreifen können. 

Um den Activity Life Cycle auch sauber zu implementieren müssen wir die Methoden ''onPause'' und ''onResume'' noch überschreiben. In dieser Rufen wir die selben Methoden auch für unsere GLSurfaceView auf. Dies ist nötig damit diese verschiedene Resourcen sauber verwalten kann. 

In unserer Activity werden wir auch gleich die Frame Rate und die Zeitspanne zwischen dem aktuellen und dem letzten Frame messen. Die Zeitspanne nennt man auch Delta Time, wieder ein Begriff den wir uns ab jetzt merken werden. Um eine genaue Zeitmessung im Millisekundenbereich zu gewährleisten verwenden wir die [http://java.sun.com/j2se/1.5.0/docs/api/java/lang/System.html#nanoTime%28%29 System.nanoTime] Methode. Diese liefert uns die aktuelle Zeit in Nanosekunden als long Typ zurück. Für die Delta Time merken wir uns den Zeitpunkt des letzten Frames als eigenen Member in der Klasse. Die Delta Time selbst errechnen wir dann in der ''onDrawFrame'' Methode indem wir einfach die aktuelle Zeit minus der zuvor gespeicherten Zeit nehmen. Diese Delta Time speichern wir in eine weitere Member Variable um später dann im Spiel einfach darauf zugreifen zu können, wir benötigen sie ja für das Frame Independant Movement. Zum Abschluss schreiben wir die aktuelle Zeit wieder in unsere dafür vorgesehene Member Variable für die nächste Delta Time Berechnung im nächsten Frame. 

Als letzten Puzzle Stein werden wir noch an unserem Design etwas failen. Wir wollen unsere Activity ja nicht jedesmal neu schreiben, darum führen wir ein eigenes Listener Konzept ein. Dies machen wir über eine simples Interface welches zwei Methoden hat:

 public interface GameListener
 {
    public void setup( GameActivity activity, GL10 gl );
    public void mainLoopIteration( GameActivity activity, GL10 gl );
 }

Der GameActivity spendieren wir eine Methode ''setGameListener'' der wir eine GameListener Implementierung übergeben können. Die Activity merkt sich diesen Listener und ruft seine Methoden entsprechend auf. Die Methode ''setup'' wird dabei nach dem Start des Spiels aufgerufen und ermöglicht es uns Resourcen zu laden die wir dann später im Main Loop gebrauchen. In der GameActivity rufen wir diese Methode in ''onSurfaceCreated'' auf, so ein GameListener gesetzt wurde. Die Methode mainLoopIteration implementiert den Körper des Main Loop. Hier werden wir später dann alles für unser Spiel nötige erledigen wie die Welt zu simulieren oder diese zu Zeichnen. Diese Methode wird in der Activity in ''onDrawFrame'' aufgerufen. Fangen wir mit der Programmierung eines neuen Spiels an implementieren wir lediglich eine Activity die direkt von GameActivity ableitet und setzen ihr einen GameListener in der onCreate Methode. Der GameListener ist also das eigentlich Spiel. 

Damit haben wir vorerst den Grundstock für unser erstes Spiel gelegt, eine voll Funktionsfähige Activity die uns die Verwendung von OpenGL erlaubt. Wir werden die GameActivity Klasse gleich noch ein wenig ausbauen um dort auch Eingaben entgegen zu nehmen. Den Source für die Klasse könnt ihr euch unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/tools/GameActivity.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/tools/GameActivity.java] ansehen.  

== Touch Screen & Accelerometer ==
Das Lesen von User Eingaben auf Android ist wie vieles anderes wieder über ein Listener Konzept implementiert. Wir vernachlässigen hier Eingaben über den Trackball, die Tastatur oder das D-Pad da dies den Rahmen dieses Artikels wohl sprengen würde. Wir konzentrieren uns zuerst auf Touch Eingaben und gehen später zum Accelerometer über.

Für die Eingabe per Touch brauchen wir ein GUI Element das diese auch entgegennimmt. Mit der GLSurfaceView in unserer GameActivity haben wir bereits einen geeigneten Kandidaten. Es gilt somit nur einen entsprechenden Listener bei der GLSurfaceView zu registrieren. Das Interface das wir implementieren müssen nennt sich [http://developer.android.com/reference/android/view/View.OnTouchListener.html OnTouchListener] und hat nur eine einzige Methode

 public abstract boolean onTouch(View v, MotionEvent event)

Für uns interessant ist der Parameter event vom Typ [http://developer.android.com/reference/android/view/MotionEvent.html MotionEvent]. Dieser beinhaltet die Koordinaten des Touch events sowie die Aktion, also ob der Finger gerade aufgesetzt wurde, ob er gezogen wird oder ob er wieder hochgegangen ist. Die Koordinaten sind dabei zweidimensional und relativ zum View für den wir den Listener registriert haben. Über die Methoden [http://developer.android.com/reference/android/view/MotionEvent.html#getX%28%29 MotionEvent.getX()] und [http://developer.android.com/reference/android/view/MotionEvent.html#getY%28%29 MotionEvent.getY()] erhalten wird die Werte. Abhängig davon ob wir im Landscape oder Portrait Modus sind sind die X und Y Achse ausgerichtet. Die positive X Achse zeigt dabei immer nach rechts, die positive Y Achse nach '''unten'''. Dies ist ein wichtiges Faktum welches vielen Beginnern am Anfang Probleme macht da es nicht mit dem in der Schule gelernten klassischen cartesischen Koordinatensystem übereinstimmt. Die Koordinaten werden dabei wieder in Pixel angegeben. 

Welche Aktion gerade aktuell ist liefert uns die Methode ''MotionEvent.getAction()''. Wir werden auf die Aktionen 
''MotionEvent.ACTION_DOWN'', ''MotionEvent.ACTION_UP'' und ''MotionEvent.ACTION_MOVE'' reagieren. Die GameActivity lassen wir das interface ''OnMotionListener'' implementieren. Wir spendieren ihr auch drei neue Member variablen, ''touchX'', ''touchY'' und ''isTouched'' in denen wir den aktuellen Status des Touch-Screen speichern. Kommt ein ''MotionEvent.ACTION_DOWN'' Event daher speichern wir die x und y Koordinate in ''touchX'' bzw. ''touchY'' und setzen ''isTouched'' auf true, bei einem ''MotionEvent.ACTION_MOVE'' machen wir das selbe und im Falle von ''MotionEvent.ACTION_UP'' setzen wir ''isTouched'' auf false. Damit wir im GameListener auf den aktuellen Status zugreifen können geben wir der GameActivity auch noch Getter Methoden um auslesen zu können. Dieses auslesen des aktuellen Status nennt man allgemein auch [http://de.wikipedia.org/wiki/Polling_%28Informatik%29 Polling]. 

Es sei angemerkt das die ''onTouch'' Methode im GUI Thread und nicht im Render Thread der GLSurfaceView vom Betriebssystem aufgerufen wird. Normalerweise müßte man sich hier Sorgen um etwaige Thread Synchronisierung machen. Da es sich bei den Member Variablen die den Status halten aber um Plain Old Datatypes handelt und das schreiben auf diese atomar ist können wir das hier einfach übersehen. 

Die GameActivity registriert sich selbst als OnTouchListener bei der GLSurfaceView in der ''onCreate'' Methode die wir dementsprechend erweitern.

Der Accelerometer ist ebenfalls wieder über ein Listener Konzept ansprechbar (ja, das zieht sich so ziemlich durch alles durch). Das entsprechende Interface nennt sich [http://developer.android.com/reference/android/hardware/SensorEventListener.html SensorEventListener]. Diesen registriert man aber nicht bei einem View sondern beim [http://developer.android.com/reference/android/hardware/SensorManager.html SensorManager]. Zugriff erhalten wir auf diesen wie folgt:

 SensorManager manager = (SensorManager)context.getSystemService(Context.SENSOR_SERVICE);

Bevor wir uns dort registrieren können müssen wir aber zuerst einmal schaun ob der Accelerometer überhaupt verfügbar ist. Dies funktioniert so:

 boolean accelerometerAvailable = manager.getSensorList(Sensor.TYPE_ACCELEROMETER).size() > 0;

Ist ein Accelerometer vorhanden können wir uns ohne große Umschweife bei diesem registrieren:

 Sensor accelerometer = manager.getSensorList(Sensor.TYPE_ACCELEROMETER).get(0);
 if(!manager.registerListener(this, accelerometer, SensorManager.SENSOR_DELAY_GAME ) )
    accelerometerAvailable = false;

Vom Manager holen wir uns zuerst den ersten Accelerometer Sensor den wir finden (in der Regel gibt es davon nur einen). Anschließend registrieren wir uns über die [http://developer.android.com/reference/android/hardware/SensorManager.html#registerListener%28android.hardware.SensorEventListener,%20android.hardware.Sensor,%20int,%20android.os.Handler%29 SensorManager.registerListener()] Methode. Dieser Vorgang kann fehlschlagen darum prüfen wir das auch. Der Parameter ''SensorManager.SENSOR_DELAY_GAME'' gibt dabei an wie oft das Betriebssystem den Accelerometer abtasten soll, in diesem Fall oft genug um für ein Spiel zu genügen. 

Was noch bleibt ist das verabeiten der Sensor Events. Das machen wir in der [http://developer.android.com/reference/android/hardware/SensorEventListener.html#onSensorChanged%28android.hardware.SensorEvent%29 SensorEventListener.onSensorChanged()] Methode die wir implementieren. 

 public abstract void onSensorChanged(SensorEvent event)

Ähnlich wie bei Touch-verarbeiten bekommen wir hier wieder ein Event, in diesem Fall vom Typ [http://developer.android.com/reference/android/hardware/SensorEvent.html SensorEvent]. Diese Klasse besitzt einen public Member namens values der die für uns relevanten Werte enthält. Derer gibt es drei an der Zahl gespeichert an den Indizes 0 bis 2. Diese drei Werte geben dabei die Beschleunigung in Meter pro Sekunde entlang der x, y und z-Achse des Android Devices an. Der maximal Wert beträgt dabei jeweils +-9.81m/s was der Erdbeschleunigung entspricht. Hält man das Android Device im Portrait mode so geht die positive x-Achse nach rechts, die positive y-Achse nach oben und die positive z-Achse gerade aus durch das Device. 

<gallery perrow=1>
Datei:Motorola_Milestone_2.jpg|Accelerometer Achsen
</gallery>

Dies bleibt auch so wenn man das Device im Landscape Modus haltet. Wir werden dann bei der Space Invaders Umsetzung sehen wie wir diese Werte ausnutzen können. 

Gleich wie für Touch Events spendieren wir der GameActivity einige neue Dinge. Zu aller erst wäre da eine neue Member Variable vom Typ float array. Diese hält unsere drei Accelerometer Werte. Weiters lassen wir die Activity das SensorEventListener Interface implementieren. Zum Abschluss bauen wir noch drei Methoden die uns jeweils den Accelerometer Wert für eine Achse liefern und wir sind fertig. Gleich wie für TouchEvents können wir damit den Accelerometer Status pollen. 

Eine Beispiel-Applikation die den aktuellen Touch- und Accelerometer-Status per LogCat ausgibt findet ihr unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/InputSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/InputSample.java]. Diese zeigt auch gleich wie wir ab jetzt neue Samples und das eigentlich Spiel aufbauen werden. Wir leiten zuerst von GameActivity ab, registrieren uns in der ''onCreate'' Methode als GameListener bei uns selbst und befüllen dann die ''setup'' und ''render'' Methode mit unserem Applikations-Code. Einfach und elegant.

== Resourcen, Assets und die SD-Karte ==
Date Ein- und Ausgabe auf Android ist ein weites Land. Mehrere Möglichkeiten stehen uns zur Verfügung. Wir werden kurz auf alle eingehen, kleine Code-Snippets sollen illustrieren wie man die einzelnen Möglichkeiten verwenden kann.

=== Resource ===
Resourcen stellen den von Google gewünschten Weg zur Verwaltung von Dateien dar. Sie werden im Android Projekt in speziel dafür vorgesehene Ordner gespeichert und sind dann im Applikations Code über Identifier direkt ansprechbar. Für die Spieleentwicklung sind sie meiner Ansicht nach nur bedingt von Nutzen da die vorgegebene Verzeichnisstruktur etwas einschränkt. Auf Resourcen gibt es nur Lesezugriff da sie direkt in der Apk-Datei der Applikation abgelegt werden, ähnlich wie Resourcen in normalen Java Jar-Dateien. Einen schönen Überblick bietet folgender [http://developer.android.com/guide/topics/resources/resources-i18n.html Link], wir lassen Resourcen einmal außen vor und wenden uns dem nächsten Kandidaten zu.

=== Assets ===
Assets werden ebenfalls wie Resourcen direkt in der APK-Datei eingepackt. Der Vorteil liegt hier in der freien Wahl der Verzeichnisstruktur. Sie ähneln damit viel mehr dem herkömmlichen Java Resourcen Mechanismus (und sind mir daher auch sympathischer). Im Android Projekt kann man unter dem Assets Verzeichnis seine eigene Struktur beliebig anlegen. Der Zugriff auf ein Asset läuft dabei gewohnt über InputStreams:

 InputStream in = activity.getAssets().open( "path/to/resource" );

Wie Resourcen sind auch Assets nur lesbar.

=== SD Karte ===
So der Besitzer des Android Devices eine SD-Karte eingelegt hat kann man in der Regel auf dieser schreiben und lesen. Dazu bedarf es im AndroidManifest.xml File des Projektes eines Zusatzes:

 <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE"/>

Ein- und Ausgabe funktioniert dann über die herkömmlichen Java Klassen:

 FileInputStream in = new FileInputStream( "/sdcard/path/to/file" );
 FileOutputStream out = new FileOutputStream( "/sdcard/path/to/file" );

== OpenGL ES ==
Jetzt geht's ans Eingemachte. [http://en.wikipedia.org/wiki/OpenGL_ES OpenGL ES] ist eine Schnittstelle die es uns erlaubt direkt mit der Grafikkarte eines mobilen Devices zu sprechen. Der Standard wurde von mehreren Herstellern gemeinsam entworfen und lehnt sich stark an die Variante an die in herkömmlichen PCs aber auch Workstations zum Einsatz kommt (genauer an die Version 1.3). Im Rahmen dieses Artikels werden wir uns die wichtigsten Dinge zu Gemüte führen, verständlicherweise kann ich hier aber nicht auf alles und jedes eingehen. Bevor wir uns in die Untiefen von OpenGL ES stürzen müssen wir uns aber noch ein paar grundlegende Dinge anschaun die allgemein in der Computergrafik gelten.

=== Grundlegendes zur Grafikprogrammierung ===
Die Entwicklung im Grafikbereich war in den letzten Jahrzehnten extrem. Viele Dinge haben sich geändert, bei der Programmierung blieb aber auch einiges gleich. Grundlage für so ziemlich jede Art von Grafikprogrammierung ist der sogenannte [http://en.wikipedia.org/wiki/Framebuffer Frame Buffer]. Dieser ist ein Teil des Video-RAM und entspricht in Java Termen einem großen eindimensionalen Array in dem die Farbwerte jedes Pixels für das aktuell am Bildschirm angezeigte Bild gespeichert werden. Wie die Farbwerte codiert werden hängt vom Bildschirmmodus ab. Hier kommt der Begriff der [http://de.wikipedia.org/wiki/Farbtiefe_%28Computergrafik%29 Farbtiefe] ins Spiel. Diese spezifiziert wieviele Bits pro Pixel verwendet werden. Herkömmlicherweise sind das bei Desktop Systemen 24- bzw. 32-Bit. Auf mobilen Devices sind 16-Bit Farbtiefen weit verbreitet. Die Farbe selbst wird als Rot-Grün-Blau Triple bzw. Rot-Grün-Blau-Alpha Quadruple in diesen 16-, 24- oder 32-Bit abgelegt. Je nach Farbtiefe kann für jede der Komponenten natürlich eine größere oder kleinere Reichweite entstehen. Wir müssen uns aber Spaghettimonster sei dank bei OpenGL ES nicht oft und vor allem nicht so intensiv wie zu DOS-Zeiten mit der Thematik auseinandersetzen. Farben werden in OpenGL ES normalerweise normiert, d.h. im Bereich zwischen 0 und 1 für jede der Komponenten der Farbe (rot, grün, blau, alpha=Transparenz) angegeben. 

Wollen wir also die Ausgabe am Bildschirm ändern so müssen wir den Framebuffer, genauer die Pixel im Framebuffer manipulieren. Früher geschah das wirklich quasi noch per Hand, heutzutage wird uns diese direkte Manipulation des Framebuffer von Bibliotheken wie OpenGL abgenommen. Grob gesagt zeichnet OpenGL für uns gefärbte, texturierte Dreiecke in den Framebuffer und das ganze Hardwarebeschleunigt. Wir haben Einfluss darauf wo im Framebuffer diese Dreiecke wie gezeichnet werden, wie wir später noch sehen. 

Pixel müssen natürlich addressiert werden können. Man verwendet dazu ein zweidimensionales Koordinaten-System. Koordinaten in diesem System werden dabei in eine lineare Addresse im Framebuffer umgerechnet mit der einfachen Formel:

 Adresse = x + y * Breite

Wobei mit Breite die Breite des Bildschirms in Pixel gemeint ist. Hier erklärt sich auch wieso die y-Achse in diesem Koordinaten-System nach unten zeigt (wie jenes das wir für Touch Events verwenden). Die Adresse 0 im Frame Buffer entspricht dem Pixel in der oberen linken Ecke des Bildschirms und hat die Koordinaten 0,0. Bei CRT-Monitoren fängt der Kathodenstrahl in dieser Ecke an, die Daten für die Intensität die er haben soll bekommt er vereinfacht gesagt aus dem Frame Buffer, wobei natürlich am Anfang dieses Frame Buffers zu lesen begonnen wird (Position 0, Koordinate 0,0).

OpenGL arbeitet intern auch in diesem Koordinaten-System, nach außen aber mit einem anderen. Unkonfiguriert liegt der Ursprung in der Mitte des Bildschirms, die positive x-Achse geht nach rechts und die positive y-Achse geht nach oben. dabei bewegen sich die x und y Koordinaten im Bereich -1, 1, ähnlich zu den Bereichen bei Farben. Wollen wir Pixel-perfekt arbeiten müssen wir das OpenGL erst beibringen. Wir werden später noch sehen wie wir das bewerkstelligen.

Bewegung am Bildschirm entsteht ähnlich wie bei einem Zeichentrickfilm. Es werden verschiedene Animationsphasen, oder Frames nacheinander in den Framebuffer geschrieben. Ist die Frequenz mit der wir die Frames schreiben hoch genug entsteht beim Betrachter die Illusion von Bewegung. 24 Bilder pro Sekunde werden in der Regel bei Filmen gezeigt. 

=== Ein wenig Mathematik ===
Ja, ohne Mathematik kommen wir leider nicht aus. Konkret brauchen wir ein wenig lineare Algebra. Klingt grauslich, ist es aber eigentlich gar nicht. Den Stoff den wir uns hier zu Gemüte führen sollten wir alle schon einmal in der Schule gehört haben. Wir werden uns kurz mit Vektoren in der dritten Dimension beschäftigen.

Definieren wir zuerst das Koordinaten-System von OpenGL in dem wir uns dann bewegen werden. Die positive x-Achse zeigt nach rechts, die positive y-Achse zeigt nach oben und die positive z-Achse zeigt aus der ebene heraus. Siehe dazu die nächste Grafik:

[[Datei:coordinatesystem.jpg|200px]]

Einen Punkt in diesem System gibt man über die Verschiebung auf den drei Achsen an, d.h. ein Punkt hat 3 Koordinaten, x, y und z. Ein Vektor gibt eine Richtung im Koordinaten-System an und ist nicht mit einem Punkt gleichzusetzen. Vektoren können im System beliebig verschoben werden. Trotzdem werden wir die Termini Vektor und Punkt ein wenig durchmischen da man im Alltag in der Regel mit Vektoren arbeitet. Wir werden im Artikel Vektoren wie folgt anschreiben:

 '''v''' = [vx, vy, vz]

Vektoren werden '''fett''' gedruckt, Skalare (also einfach Zahlen) werden wir normal drucken. 

Mit Vektoren kann man auch wunderbar rechnen. Als erstes wollen wir die Länge eines Vektors bestimmen:

 |'''v'''| = Math.sqrt( vx * vx + vy * vy + vz * vz );

Das sollte euch bekannt vor kommen: die Länge eines Vektors leitet sich von Pythagoras' Satz ab. Die Notation auf der rechten Seite bedeutet "Länge des Vektors '''v'''". 

Vektoren kann man auch addieren und subtrahieren:

 '''a''' + '''b''' = [ax + bx, ay + by, az + bz]
 '''a''' - '''b''' = [ax - bx, ay - by, az - bz]

Bei der Multiplikation sieht das ganz ähnlich aus:

 '''a''' * '''b''' = ax * bx + ay * by + az * bz

Das nennt man auch das [http://de.wikipedia.org/wiki/Skalarprodukt Skalarprodukt] zweier Vektoren. Mit einem kleinen Kniff kann man mit diesem Skalarprodukt den Winkel zwischen zwei Vektoren messen:

 winkel = Math.acos( '''a''' * '''b''' / ( |'''a'''| * |'''b'''| ) );

Ich mische hier Java und mathematische Notation etwas da mir die Formatierungsmöglichkeiten fehlen und es so etwas verständlicher wird. Der Winkel ist dabei immer <= 180 Grad. ''Math.acos()'' liefert diesen Winkel jedoch nicht in Grad sondern in Bogenmaß welches man recht einfach mit ''Math.toDegrees()'' in Grad umrechnen kann. Alle trigonometrischen Funktionen der Klasse Math arbeiten übrigens mit Bogenmaß, sowohl was Parameter als auch was Rückgabewerte betrifft. Das sorgt oft für schwer zu findende Bugs, also immer daran denken.

Zu allerletzt wollen wir noch auf Einheitsvektoren eingehen. Dies sind Vektoren die die Länge eins haben. Um einen beliebigen Vektor zu einem Einheitsvektor zu machen müssen wir dessen Komponenten einfach durch seine Länge dividieren.

  '''a'''' = [ax / |'''a'''|, ay / |'''a'''|, az / |'''a'''|]

Der Apostroph nach dem '''a''' zeigt an dass es sich um einen Einheitsvektor handelt. Wir werden Einheitsvektoren später für ein paar Kleinigkeiten benötigen. 

Und damit sind wir mit dem Mathematik Kapitel auch schon fertig. Ich hoffe es war nicht gar zu schlimm. Bei Unsicherheiten empfehle ich euch im Netz ein wenig in Material zum Thema zu stöbern. 

=== Das erste Dreieck ===
Wie Eingangs schon erwähnt ist OpenGL im Grunde seines Herzens eine Dreieck-Zeichenmaschine. In diesem Abschnitt wollen wir uns dran machen das erste Dreieck auf den Bildschirm zu Zaubern. Dazu erstellen wir eine neue Activity die von GameActivity ableitet und die sich selbst als GameListener in der ''onCreate()'' Methode registriert. 

Wie wir schon Eingangs erwähnt haben nennen sich die Dinge die wir zeichnen Meshes. Ein solches Mesh wird durch sogenannte Vertices definiert. Ein Vertex entspricht dabei einem Punkt des Meshes mit verschiedenen Attributen. Anfangs wollen wir uns nur um die wichtigste Komponente kümmern, der Position. Die Position eines Vertex wird, ihr habt es erraten, als dreidimensionaler Vektor angegeben. Ein Punkt alleine macht noch kein Mesh, darum brauchen wir mindestens drei. Mehrere Dreiecke sind natürlich auch kein Problem, wir wollen aber klein anfangen. 

OpenGL ES erwartet in seiner Basisversion 1.1 die Vertex Positionen in einem [http://java.sun.com/j2se/1.4.2/docs/api/java/nio/ByteBuffer.html direct ByteBuffer]. Definieren wir zur Übung einmal ein Dreieck in der x-y-Ebene mit Hilfe so eines ByteBuffers:

 ByteBuffer buffer = ByteBuffer.allocateDirect( 3 * 3 * 4 );
 buffer.order(ByteBuffer.nativeOrder());
 FloatBuffer vertices = buffer.asFloatBuffer();
 vertices.put( -0.5f );
 vertices.put( -0.5f );
 vertices.put( 0 );		
 vertices.put( 0.5f );
 vertices.put( -0.5f );
 vertices.put( 0 );	
 vertices.put( 0 );
 vertices.put( 0.5f );
 vertices.put( 0 );

Ganz schön viel Code für so ein kleines Dreieck. Als erstes erstellen wir einen direct ByteBuffer der 3 * 3 * 4 Bytes groß ist. Die Zahl ergibt sich da wir 3 Vertices haben zu je 3 Komponenten (x, y, z) die jeweils 4 Byte Speicher brauchen (float -> 32-bit). Anschließend sagen wir dem ByteBuffer, dass er bitte alles in nativer Ordnung speichern soll, d.h. in Big- oder Little-Endian. Den fertig initialisierten ByteBuffer wandeln wir dann in einen FloatBuffer um den wir mit der Methode ''FloatBuffer.put()'' befüllen können. Jeweils 3 Aufrufe definieren die Koordinaten eines Vertex' unseres Dreiecks. Der erste Vertex liegt links unter dem Ursprung, der zweite Vertex rechts unter dem Ursprung und der dritte Vertex direkt über dem Ursprung: 

[[Datei:dreieck.jpg|200px]]

Damit haben wir OpenGL aber noch immer nicht wirklich etwas verraten. Das machen wir doch gleich und veranlassen das Zeichnen unseres Dreiecks:

 gl.glViewport(0, 0, activity.getViewportWidth(), activity.getViewportHeight());
 gl.glEnableClientState(GL10.GL_VERTEX_ARRAY );    
 gl.glVertexPointer(3, GL10.GL_FLOAT, 0, vertices);
 gl.glDrawArrays(GL10.GL_TRIANGLES, 0, 3);

Im ersten Methoden Aufruf teilen wir OpenGL mit welcher Bereich des Bildschirms bezeichnet werden soll. Die ersten beiden Parameter geben dabei die Startkoordinaten des zu bezeichnenden Bereichs an, die nächsten beiden seine Größe. Beide angaben werden in Pixeln gemacht, hier sagen wir konkret dass der ganze Bildschirm ausgenutzt werden soll. 

'''Achtung:''' der Emulator benötigt unbedingt den Aufruf von ''glViewport''. Auf Devices ist der Viewport bereits auf den gesamten Bildschirm gesetzt, im Emulator hat der Viewport am Anfang die Größe 0,0. Nicht vergessen da sonst nichts am Bildschirm gezeichnet wird und man stundenlang sucht (ja, ist mir auch schon passiert...)!

Im nächsten Aufruf sagen wir OpenGL dass wir ihm jetzt gleich Vertex Positionen übergeben werden und er fortan beim Zeichnen immer den Übergebenen FloatBuffer verwenden soll. Im dritten Aufruf teilen wir OpenGL mit wo er die Positionensdaten findet. Der erste Parameter gibt dabei die Anzahl der Vertices an, der zweite gibt den Typen der Komponenten jeder Position an, in diesem Fall floats. Der dritte Parameter nennt sich stride und ist für uns ohne Belang, wir setzen ihn einfach auf 0. Der letzte Parameter ist der FloatBuffer den wir zuvor mit den Positionen der 3 Vertices befüllt haben. Als letztes befehlen wir OpenGL die eben definierten Vertices zu zeichnen. Der erste Parameter gibt dabei an was wir zeichnen wollen, und zwar Dreiecke. Der zweite Parameter gibt an ab welcher Position im FloatBuffer OpenGL beginnen soll die Positionsdaten zu holen. Der letzte Parameter sagt OpenGL noch wieviele Vertices wir gezeichnet haben wollen. Zeichnen wir Dreiecke muss dieser Parameter immer ein Vielfaches von 3 sein. Und schon haben wir das erste Dreieck mit OpenGL gezeichnet! Zur Entspannung hier der gesamte Code dieses Beispiels:

 public class TriangleSample extends GameActivity implements GameListener 
 {
    private FloatBuffer vertices;	
    public void onCreate( Bundle savedInstance )
    {
       super.onCreate( savedInstance );
       setGameListener( this );
    }	
    @Override
    public void setup(GameActivity activity, GL10 gl) 
    {
       ByteBuffer buffer = ByteBuffer.allocateDirect( 3 * 4 * 3 );
       buffer.order(ByteOrder.nativeOrder());
       vertices = buffer.asFloatBuffer();
       vertices.put( -0.5f );
       vertices.put( -0.5f );
       vertices.put( 0 );
       vertices.put( 0.5f );
       vertices.put( -0.5f );
       vertices.put( 0 );	
       vertices.put( 0 );
       vertices.put( 0.5f );
       vertices.put( 0 );	
       vertices.rewind();
    }	
    @Override
    public void mainLoopIteration(GameActivity activity, GL10 gl) 
    {	
       gl.glViewport(0, 0, activity.getViewportWidth(), activity.getViewportHeight());
       gl.glEnableClientState(GL10.GL_VERTEX_ARRAY );    
       gl.glVertexPointer(3, GL10.GL_FLOAT, 0, vertices);
       gl.glDrawArrays(GL10.GL_TRIANGLES, 0, 3);
    }	
 }

Alternativ kann man sich den Source etwas schöner formartiert unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TriangleSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TriangleSample.java] ansehen.

Und hier ein Screenshot unseres Dreiecks

[[Datei:triangle.png]]

=== Farbspiele ===
Ein weisses Dreieck ist natürlich etwas langweilig. Um das zu ändern verwenden wir bevor wir ''glDrawArrays'' aufrufen den Befehl

 glColor4f( float r, float g, float b, float a );

R, g, b stehen für die drei Farbkomponenten, a steht für die transparenz. Alle Werte sind im Bereich 0 bis 1 anzugeben. Setzen wir r und b auf 1 bekommen wir ein schönes pink:

[[Datei:pink.png]]

Ich habe vorher schon erwähnt das ein Vertex nicht nur eine Position hat. Solange wir diese nicht explizit definieren hat jeder Vertex in einem Mesh die Farbe die wir mit ''glColor4f'' angeben. Um jedem Vertex eine eigene Farbe zu geben verwenden wir den selben Mechanismus wie für die Vertex Positionen. Zuerst bauen wir wieder einen direct FloatBuffer in den wir für jeden Vertex dir r, g, b und a Werte speichern:

 buffer = ByteBuffer.allocateDirect( 3 * 4 * 4 );
 buffer.order(ByteOrder.nativeOrder());
 colors = buffer.asFloatBuffer();	
 colors.put( 1 );
 colors.put( 0 );
 colors.put( 0 );
 colors.put( 1 );
 colors.put( 0 );
 colors.put( 1 );
 colors.put( 0 );
 colors.put( 1 );	
 colors.put( 0 );
 colors.put( 0 );
 colors.put( 1 );
 colors.put( 1 );	
 colors.rewind();

Da wir 3 Vertices haben brauchen wir einen ByteBuffer der 3 Farben hält, zu je 4 Komponenten (r, g, b, a) mit je 4 byte (floats). Den ByteBuffer schalten wir wieder auf native order und wandeln ihn in einen FloatBuffer um. Nun können wir ihn mit den drei Farben für unsere drei Vertices befüllen, hier rot (1, 0, 0, 1), grün (0, 1, 0, 1) und blau (0, 0, 1, 1). Beim Zeichnen sagen wir OpenGL das es bitte unseren FloatBuffer für die Farben der Vertices verwenden soll:

 gl.glEnableClientState(GL10.GL_COLOR_ARRAY );
 gl.glColorPointer( 4, GL10.GL_FLOAT, 0, colors );

Zuerst sagen wir OpenGL das wir für die Farben der einzelnen Vertices einen FloatBuffer haben (''glEnableClientState''). Dann geben wir gleich wie bei den Vertex Positionen an wo dieser FloatBuffer zu finden ist (''glColorPointer''). Der erste Parameter sagt wieviele Komponenten eine Farbe hat (4 -> r, g, b, a), der zweite Parameter gibt an welchen Typ die Komponenten haben, der dritte Parameter ist wieder der stride und der vierte ist unser zuvor befüllter FloatBuffer. Und das wars auch schon wieder. '''Achtung:''' hat man den client state GL10.GL_COLOR_ARRAY enabled so wird jeder Aufruf von ''glColor4f'' ignoriert. Es muss dann unbedingt ein FloatBuffer mit ''glColorPointer'' angegeben werden der zumindest soviele Farben besitzt wie das Mesh Vertices hat, bzw. soviele wie man Vertices bei ''glDrawArrays'' angibt!

Der gesamte Code zum Zeichnen unseres nun schön eingefärbten Dreiecks schaut so aus:

 gl.glEnableClientState(GL10.GL_VERTEX_ARRAY );    
 gl.glVertexPointer(3, GL10.GL_FLOAT, 0, vertices);
 gl.glEnableClientState(GL10.GL_COLOR_ARRAY );
 gl.glColorPointer( 4, GL10.GL_FLOAT, 0, colors );
 gl.glDrawArrays(GL10.GL_TRIANGLES, 0, 3);

Hier zeichnet sich langsam ein Muster ab: für jede Komponente eines Vertex, also z.B. die Position oder die Farbe, enablen wir einen Array (GL10.GL_VERTEX_ARRAY, GL10.GL_COLOR_ARRAY) mit ''glEnableClientState'' und geben dann mit einer der ''glXXXPointer'' Methoden an wo der entsprechende "Array" (in unserem Fall in Form eines FloatBuffer) zu finden ist. Diese Methode des Zeichnens nennt man in OpenGL [http://www.opengl.org/documentation/specs/version1.1/glspec1.1/node21.html Vertex Arrays] und ist die einzige Methode mit der man in OpenGL ES 1.0 überhaupt etwas zeichnen kann. Wir werden vielleicht in einem anderen Artikel die sogenannten [http://www.spec.org/gwpg/gpc.static/vbo_whitepaper.html Vertex Buffer Objects] näher betrachten die ab OpenGL ES 1.1 zur Verfügung stehen. Einstweilen bleiben wir aber bei den Vertex Arrays da die auf allen Android Devices funktionieren.

Hier noch ein Screenshot unseres farbigen Dreiecks:

[[Datei:color.png]]

Den Code für dieses Beispiel könnt ihr euch unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/ColorSample2.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/ColorSample2.java] ansehen.

=== Texturen ===
So richtig peppig wird's wenn man seinen Dreiecken [http://en.wikipedia.org/wiki/Texture_mapping Texturen] verpasst. Dabei tapeziert man auf die Dreiecke eine Bitmap die man zuvor geladen hat. Bevor wir uns an die Texturierung selbst machen schaun wir uns schnell an wie man eine Bitmap überhaupt ladet. Im Beispiel Projekt habe ich im assets Verzeichnis ein PNG-File namens "droid.png" abgelegt. Dieses laden wir in unserer ''setup'' Methode wie folgt:

 try
 {
    Bitmap bitmap = null;
    bitmap = BitmapFactory.decodeStream( getAssets().open( "droid.png" ) );
 }
 catch( Exception ex )
 {
    // Oh no!
 }

Sehr einfach: wir rufen die statische Methode ''decodeStream'' der Klasse BitmapFactory auf und übergeben ihr einen InputStream auf unser Bitmap Asset namens "droid.png". Nachdem die Methode eine IOException wirft baun wir noch ein try-catch drum. Da wir klug genug waren das Asset auch wirklich in das entsprechende Verzeichnis zu packen sollte keine Exception fallen. Normalerweise handle ich Exceptions beim Resourcen laden mit einem Log Output und einem ''System.exit(-1)''. Wie ihr das löst bleibt aber euch überlassen.

Das Texturieren selbst ist wieder relativ einfach. Die Bitmap die man läd wird in ein normiertes Koordinaten-System gelegt:

[[Datei:texturecoordinates.png]]

Den Achsen geben wir zur Vermeidung von Verwechslungen mit dem Vertex Positions Koordinaten-System neue Namen, s nach Rechts und t nach unten. Egal welche Abmessungen das Bild hat, Pixel werden immer im Bereich [0,0]-[1,1] angesprochen. Man kann so z.B. leicht eine hochauflösende Texture mit einer niedrig auflösenden Texture austauschen ohne die Texture-Koordinaten des Meshes zu ändern. Was die Bildabmessungen betrifft so gibt es eine Limitation auf Android: diese müssen 2er Potenzen sein, also 1, 2, 4, 8, 32, 64, 128, 256 usw. Maximal sollte man nicht mehr als 512x512 Pixel verwenden, die Hardware könnte das nicht mehr unterstützen. Die Bilder müssen dabei nicht quadratisch sein sondern können z.B. auch die Abmessungen 32x64 oder 128x32 haben. 

Damit unser Dreieck texturiert wird müssen wir für jeden Vertex zuerst eine Texture-Koordinate angeben. Die Angabe erfolgt dabei im Koordinaten-System der Texture, also zweidimensional und jeweils zwischen 0 und 1 (man kann auch kleinere und größere Werte angeben, das schaun wir uns aber später an):

[[Datei:texturecoordinates2.png]]

Hier haben wir unser Dreieck gemapped. Aufmerksame Leser wissen schon was jetzt kommt: Die Koordinaten stopfen wir wieder in einen FloatBuffer:

 buffer = ByteBuffer.allocateDirect( 3 * 2 * 4 );
 buffer.order(ByteOrder.nativeOrder());
 texCoords = buffer.asFloatBuffer();	
 texCoords.put(0);
 texCoords.put(1);		
 texCoords.put(1);
 texCoords.put(1);		
 texCoords.put(0.5f);
 texCoords.put(0);
 texCoords.rewind();

Die größe ergibt sich wieder aus den 3 Vertices für die wir jeweils Texture-Koordinaten mit 2 Komponenten haben die wiederum jeweils 4 Byte groß sind (float). Der Rest sollte selbst erklärend sein. 

Bevor wir die Texture-Koordinaten als Vertex Komponente OpenGL mitteilen müssen wir uns noch um eine Kleinigkeit kümmern: das eigentliche laden der Texture. Wir haben zwar schon die Bitmap aus dem Asset geladen, eine Texture haben wir aber noch nicht erstellt. Das machen wir jetzt:

 int[] textureIds = new int[1];
 gl.glGenTextures(1, textureIds, 0);		
 textureId = textureIds[0];

Mit ''glGenTextures'' weisen wir OpenGL an uns eine neue Texture zu erstellen. Der erste Parameter gibt dabei an wieviele Texturen wir erstellen wollen (eine), in den zweiten Parameter speichert OpenGL dann die ID(s) der neuen Texture(n). Der letzte Parameter ist nur ein Offset ab dem OpenGL in dem übergebenen Array schreiben soll. Die erhaltene Texture-ID müssen wir uns merken, mit dieser aktivieren wir dann später die Texture. 

Als nächstes müssen wir die Bitmap in die Texture laden. Hier hat uns das Android Team einen großen Brocken Arbeit abgenommen und stellt uns die Klasse GLUtils zur Verfügung:

 gl.glBindTexture( GL10.GL_TEXTURE_2D, textureId );
 GLUtils.texImage2D( GL10.GL_TEXTURE_2D, 0, bitmap, 0);

Zuerst müssen wir die Texture "binden" damit sie zur aktuel aktiven Texture wird. Dazu übergeben wir als ersten Parameter GL10.TEXTURE_2D (dessen Erklärung ich mir spare, das ist einfach immer so :)) und als zweiten Parameter die zuvor generierte Texture-ID. Erst dann können wir Daten in die Texture laden, ihre Konfiguration ändern oder sie als Texture für eines unserer Meshes verwenden. Als nächster Rufen wir die statische Methode ''texImage2D'' der Klasse GLUtils auf die unsere zuvor geladene Bitmap in die Texture läd. Den ersten Parameter ignorieren wir wieder, den zweiten auch (gibt den [http://de.wikipedia.org/wiki/Mip_Mapping MipMap-Level] an), als dritten übergeben wir die Bitmap und den letzten Parameter ignorieren wir auch wieder. Damit hat unsere Texture jetzt die Bilddaten die sie haben soll. 

Als letzten Schritt müssen wir die Texture jetzt noch konfigurieren:

 gl.glTexParameterf(GL10.GL_TEXTURE_2D, GL10.GL_TEXTURE_MIN_FILTER, GL10.GL_LINEAR );
 gl.glTexParameterf(GL10.GL_TEXTURE_2D, GL10.GL_TEXTURE_MAG_FILTER, GL10.GL_LINEAR );
 gl.glTexParameterf(GL10.GL_TEXTURE_2D, GL10.GL_TEXTURE_WRAP_S, GL10.GL_CLAMP_TO_EDGE );
 gl.glTexParameterf(GL10.GL_TEXTURE_2D, GL10.GL_TEXTURE_WRAP_T, GL10.GL_CLAMP_TO_EDGE );	

Die ersten beiden Methoden setzen die Filter der aktuel gebundenen Texture die zum Einsatz komm wenn die Texture am Bildschirm größer als im Original ist, bzw. kleiner als im Original ist. Der letzte Parameter gibt dabei den Filter an. Hier hat man die Wahl zwischen GL10.GL_NEAREST, GL10.GL_LINEAR, GL10.GL_LINEAR_MIPMAP_NEAREST und GL10.GL_LINEAR_MIPMAP_LINEAR. GL10.GL_NEAREST ist der häßlichste, GL10.GL_LINEAR ist ein bi-linearer Filter der ganz gute Ergebnisse bringt und die beiden Mip-Map Filter ignorieren wir einstweilen wieder.

Die beiden anderen Methoden geben an was geschehen soll wenn der User Texture-Koordinaten angibt die kleiner als 0 oder größer als 1 sind. Wir wählen hier GL10.GL_CLAMP_TO_EDGE was zur Folge hat das solche Texturen einfach auf den Bereich geschnitten werden ( kleiner 0 wird 0, größer 1 wird 1 ). Alternativ kann man hier GL10.GL_WRAP angeben. Dies hat zur Folge das die Koordinaten modulo 1 genommen werden. Eine 4.5 wird so zur 0.5 und so weiter. Damit kann man die Texture über ein Dreieck mehrere Male wiederholen. Die Angabe des Wrap-Modus erfolgt dabei für die s unt t Komponente einzeln. Man kann also auf s z.B. clampen und auf t wrappen.

Damit haben wir die Texture fertig geladen und konfiguriert. Uns bleibt noch das zeichnen mit der Texture über. Hier der gesamte Code im Überblick:

 gl.glEnable( GL10.GL_TEXTURE_2D );
 gl.glBindTexture( GL10.GL_TEXTURE_2D, textureId );		
 gl.glEnableClientState(GL10.GL_TEXTURE_COORD_ARRAY );
 gl.glTexCoordPointer(2, GL10.GL_FLOAT, 0, texCoords );
 gl.glEnableClientState(GL10.GL_VERTEX_ARRAY );    
 gl.glVertexPointer(3, GL10.GL_FLOAT, 0, vertices);		
 gl.glDrawArrays(GL10.GL_TRIANGLES, 0, 3);

Zuerst müssen wir OpenGL sagen das es ab jetzt alle Meshes mit der aktuel gebundenen Texture texturieren soll. Als nächstes binden wir unsere Texture. Dann geben wir an dass unsere Vertices Texture-Koordinaten haben und wir die gleich übergeben was im nächsten Aufruf mit ''glTexCoordPointer'' geschieht. Hier unser texturiertes Dreieck:

[[Datei:texturetri.png]]

Den Code zum Beispiel findet ihr unter [[http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TextureSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TextureSample.java]

Es sei noch angemerkt dass man die Texture nur einmal baut (z.B. in der ''setup'') Methode. Ich hatte da schon Code von einigen Leuten gesehen die die selbe Texture immer und immer wieder bauen. Wie für Meshes gilt: einmal bauen, solang verwenden wie nötig, dann die Resourcen wieder freigeben. Im Fall von Vertex Arrays gibts nichts zu tun. Im Fall von Texturen müssen wir diese löschen was sehr einfach geht:

 int[] textureIds = { textureId };
 gl.glDeleteTextures( 1, textureIds, 0 );

Wir geben einfach die Texture-ID an und schon ist die Texture Geschichte. Man sollte ein gelöschte Texture natürlich nach dem Löschen nicht mehr binden. 

Und das war's wieder. Eigentlich keine Rocket Science, ein wenig Code ist es aber schon. Wir werden darum zwei Klassen bauen die uns für Meshes und Textures ein wenig Arbeit abnehmen und den Code schlanker machen. 

=== Mesh & Texture Klasse ===
Für euer Seelenheil hab ich zwei Klassen gestrickt die ihr sehr einfach verwenden könnt. Zum einen haben wir da die Mesh Klasse:

 public final class Mesh
 {
    public enum PrimitiveType
    {
       Points,
       Lines,
       Triangles,
       LineStrip,
       TriangleStrip,
       TriangleFan
    }	
    public Mesh( GL10 gl, int numVertices, boolean hasColors, boolean hasTextureCoordinates, boolean hasNormals )	
    public void render( PrimitiveType type )	
    public void vertex( float x, float y, float z )
    public void color( float r, float g, float b, float a )	
    public void normal( float x, float y, float z )	
    public void texCoord( float s, float t )
}

Ihr könnt sie über den Konstruktor einfach instanzieren. Den ersten Parameter erhaltet ihr in der ''GameListener.setup()'' bzw. ''GameListener.render()'' Methode. Der zweite Parameter gibt an wieviele Vertices das Mesh insgesamt haben soll. Der dritte Parameter besagt ob das Mesh auch Colors definiert, der vierte ob Texture-Koordinaten dabei sein sollen und der vierte ob [http://www.flipcode.com/archives/Vertex_Normals.shtml Normalen] vorhanden sind. Moment, Normalen? Die erklären wir an dieser Stelle nicht. Sie werden für die Beleuchtung von Meshes durch Lichtquellen benötigt. Damit wir in späteren Teilen einmal darauf eingehen können habe ich sie gleich in den Source miteingebaut.

Nachdem ihr das Mesh instanziert habt könnt ihr es sehr einfach befüllen. Unser Color Sample von oben würde z.B. so ausschaun:

 mesh = new Mesh( gl, 3, true, false, false );
 mesh.color( 1, 0, 0, 1 );
 mesh.vertex( -0.5f, -0.5f, 0 );
 mesh.color( 0, 1, 0, 1 );
 mesh.vertex( 0.5f, -0.5f, 0 );
 mesh.color( 0, 0, 1, 1 );
 mesh.vertex( 0, 0.5f, 0);

Als Richtlinie gilt hier: zuerst immer alle Komponenten ungleich der Position für einen Vertex angeben (Color, Texture-Koordinaten, Normale) und zum fixieren des Vertex ''vertex'' mit der Position des Vertex aufrufen. Natürlich solltet ihr nicht mehr Vertices definieren als ihr im Konstruktor angegeben habt. Zum Rendern des Mesh reicht folgender Aufruf

 mesh.render(PrimitiveType.Triangles);
 
PrimitiveType ist wie oben zu sehen ein enum welches mehrere Arten von Primitiven definiert. Wir haben bis jetzt nur die Dreiecke besprochen, es sind aber auch andere Primitive möglich. Ihr könnt diese im Netz nachschlagen (z.B. Triangle Strip) um einen Einblick zu erlangen. 

Ein schönes Feature der Klasse ist das ihr das Mesh nachdem ihr es einmal gerendert habt wieder neu definieren könnt. Ihr verwendet dazu einfach wieder die Methoden ''color'', ''texCoord'' usw. wie vorher gezeigt. Wichtig dabei ist aber, dass das Mesh mindestens einmal zuvor gerendert wurde da ansonsten ein interner Zeiger noch zurückgesetzt wird. Ihr könnt euch den Code zur Mesh Klasse unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/tools/Mesh.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/tools/Mesh.java] ansehen. Wirklich was neues mache ich dort nicht. Die Grundlagen dafür habt ihr bereits oben gesehen. Den Source Code zu einem Beispiel Programm welches die Mesh Klasse verwendet findet ihr unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/MeshSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/MeshSample.java]. Durch die Verwendung der Klasse wird der Code um einiges schlanker und verständlicher.

Die Texture Klasse ist noch einfacher:

 public class Texture 
 {	
    public enum TextureFilter
    {
       Nearest,
       Linear,
       MipMap
    }	
    public enum TextureWrap
    {
       ClampToEdge,
       Wrap
    }	
    public Texture( GL10 gl, Bitmap image, TextureFilter minFilter, TextureFilter maxFilter, TextureWrap sWrap, TextureWrap tWrap )	
    public void bind(  )	
    public void dispose( )	
    public void draw( Bitmap bmp, int x, int y )	
    public int getHeight() 
    public int getWidth() 
}

Beim Instanzieren geben wir wieder die GL10 Instanz an, gleich wie beim Mesh. Außerdem übergeben wir die Bitmap, die gewünschten Vergrößerungs- und Verkleinerungs-Filter sowie die Wrap Modi für die s und t Texture-Koordinaten. Diese haben wir ja oben ganz kurz angerissen. Auch Mip-Mapping ist hier schon implementiert, einfach den minFilter auf TextureFilter.MipMap setzen. Des weiteren gibt es eine Methode ''bind()'' die die Texture bindet, gleich wie ''glBindTexture''. Die Methode ''dispose'' löscht die Texture und gibt alle Resourcen frei. Die Methode ''draw()'' ist ein sehr nettes Feature: Sie erlaubt es im Nachhinein eine andere Bitmap an eine bestimmte x, y Position in der Texture zu zeichnen. Die Koordinaten werden dabei in Pixel angegeben, der Ursprung ist das obere linke Eck, die positive y-Achse geht nach unten. Intern bindet die Methode die Texture vor dem zeichnen, man muss hier also auf den Seiteneffekt achten. 

Was die Klasse nicht macht ist das Einschalten von Texturierung über ''glEnable''. Darauf also nicht vergessen. Ein Beispiel für die Verwendung der Texture Klasse in Zusammenhang mit der Mesh Klasse findet ihr unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TextureMeshSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/TextureMeshSample.java] anschaun. Das Mesh dort verwendet Colors und Texture-Coordinates was einen hübschen Effekt hat :)

Damit haben wir jetzt zwei sehr kleine und feine Klassen die uns viel Arbeit und Code abnehmen. 

=== Projektionen ===
Ich bin ein wenig fies. Nach dem Kapitel über Vektoren hab ich versprochen dass das alles an Mathematik war was wir hier sehen werden. Ich hab gelogen. Wir werden uns jetzt mit [http://de.wikipedia.org/wiki/Zentralprojektion Projektionen] beschäftigen. Dabei gibt es zwei für uns relevante Arten:

* [http://de.wikipedia.org/wiki/Parallelprojektion Parallelprojektion]
* [http://de.wikipedia.org/wiki/Zentralprojektion Zentralprojektion]

Die Parallelprojektion wird auch orthographische Projektion genannt, die Zentralprojektion kennt man auch als perspektivische Projektion. Was genau macht eine Projektion? Sie nimmt unsere dreidimensionalen Vertex Positionen und transformiert diese in 2D Koordinaten am Bildschirm (vereinfacht ausgedrückt, bis zu den Bildschirmkoordinaten gibts noch ein paar Zwischenschritte, die sparen wir uns aber). Die orthographische Projektion verwendet man im allgemeinen wenn man in 2D Arbeiten möchte, wie z.B. in alten NES Spielen. Die perspektivische Projektion verwendet man für alles Spiele die einen 3D Eindruck verwenden wollen. In der Schule sollten die meisten von euch schon mal Fluchtpunkt Zeichnungen gemacht haben, genau das selbe Prinzip verwendet auch die perspektivische Projektion, nur mathematisch ausformuliert. 

Beginnen wir mit der orthographischen Projektion. Für jede Art von Projektion brauchen wir eine Projektionsfläche, in unserem Fall ist das der Bildschirm. Die orthographische Projektion nimmt einfach jeden Vertex her und ignoriert dessen z-Koordinate. Die x und y Koordinate werden mehr oder minder so übernommen wie sie sind. Geometrisch kann man sich das so vorstellen, dass von jedem Vertex eine Linie ausgeht die die Projektionsfläche schneidet. Diese Linien sind alle parallel zueinander und normal, d.h. in einem 90 Grad Winkel zur Projektionsfläche. Die Schnittpunkte der Linien mit der Projektionsfläche ergeben die finalen projizierten Punkte. Ein Bild sagt mehr als tausend Worte:

[[Datei:ortho.png]]

Es ist also vollkommen unerheblich wie weit ein Punkt von der Projektionsfläche entfernt ist. Die orthographische Projektion werden wir für all unsere 2D Bedürfnisse verwenden. Wir konfigurieren sie so das wir direkt in Bildschirmkoordinaten Arbeiten können. Dazu verwenden wir die Klasse GLU die eine statische Methode namens ''glOrtho2D'' besitzt. Um das gewünschte 2D Koordinaten System zu bekommen rufen wir folgendes auf:

 gl.glMatrixMode( GL10.GL_PROJECTION );
 gl.glLoadIdentity();
 GLU.glOrtho2D( gl, 0, activity.getViewportWidth(), 0, activity.getViewportHeight() )
 
Zuerst sagen wir OpenGL dass wir die Projektions-Matrix ab jetzt bearbeiten wollen. Projektionen sind Transformationen von Vertices und werden in OpenGL über Matrizen abgebildet. Jeder Vertex den wir an OpenGL schicken wird mit ein paar Matrizen multipliziert um seine finale Position am Bildschirm zu errechnen. Die Projektionsmatrix ist eine dieser Matrizen. Wir brauchen uns aber Gott sei dank hier nicht mit Matrizen direkt herumschlagen. Als nächstes laden wir eine Einheits-Matrize. Man stelle sich hier einfach vor, dass der Inhalt der Projektions-Matrize dadurch gelöscht wird und die Matrize keinen Einfluß auf unsere Vertices hat. Die Multiplikation mit dieser Matrize ergibt den selben Vektor. Abschließend verwenden wir ''glOrtho2D'' welches eine orthographische Projektions-Matrize läd. Die Parameter geben dabei an wie groß die Projektionsfläche ist (stimmt so nicht ganz). Wir geben hier den gesamten Bildschirm an, die Angaben sind in Pixel. Ab nun können wir die Vertex Positionen in Bildschirmkoordinaten angeben wobei das Koordinaten-System wie folgt im Portrait und Landscape Modus ausschaut:

[[Datei:ortho2.png|400px]]

Zur Veranschaulichung hier noch das ganze mit einem Mesh welches wir in diesem neuen Koordinaten-System so definieren:

 mesh = new Mesh( gl, 3, false, false, false );
 mesh.vertex( 0, 0, 0 );
 mesh.vertex( 50, 0, 0 );
 mesh.vertex( 25, 50, 0 );	

Wir erwarten also ein Dreieck welches unten links neben dem Ursprung in Erscheinung tritt. Und das tut es auch (siehe Sample unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/OrthoSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/OrthoSample.java]

[[Datei:orthotri.png]]

Damit könnten wir jetzt schon fast unser erstes kleines 2D Spielchen implementieren. Da wir aber moderne Menschen sind wollen wir etwas in 3D machen. Dazu benötigen wir die perspektivische Projektion. 

Die perspektivische Projektion ist um ein Stückchen schwerer zu durchschaun als die orthographische Projektion funktioniert aber nach einem ähnlichen Prinzip. Wieder schicken wir Linien durch alle Vertices, diesmal jedoch nicht normal zur Projektionsfläche sondern durch einen Punkt vor der Projektionsfläche (auf der anderen Seite sind die Vertices). 

[[Datei:perspective.jpg]]

Dieser Punkt ist insofern besonders als dass er der Position des Auges eines Betrachters in unserer dreidimensionalen Welt entspricht. 

[[Datei:perspective2.gif]]

Die perspektivische Projektion wird durch mehrere Parameter definiert. Zum einen durch das sogenannte Field of View. Dies ist das Sichtfeld das man in der y-Achse bis zur Projektionsfläche abdeckt. Als nächstes gibt es die near und die far Clipping Planes. Die near Clipping Plane ist unserer Projektionsebene, man gibt hier die Entfernung zum Betrachter an. Die far Clipping Plane ist jene Ebene ab der nichts mehr dargestellt wird. Jeder Vertex der hinter dieser Ebene liegt wird nicht gezeichnet. Als letzten Parameter für eine perspektivische Projektion braucht man das Verhältnis zwischen Breite und Höhe der Projektionsebene, auch Aspect Ratio genannt. Diesen errechnen wir aus der Viewport Größe die in Pixeln angegeben ist. All diese Parameter gemeinsam definieren einen Sichtkegel der vorne durch die near Clipping Plane und hinten durch die Far Clipping Plane begrenzt ist. Auch ist er oben und unten sowie links und rechts begrenzt. Diesen Sichtkegel nennt man View Frustum. Ganz schön viel Information auf einmal, schaun wir uns an wie einfach das in OpenGL geht:

 gl.glMatrixMode( GL10.GL_PROJECTION );
 gl.glLoadIdentity();
 float aspectRatio = (float)activity.getViewportWidth() / activity.getViewportHeight();
 GLU.gluPerspective( gl, 67, aspectRatio, 1, 100 );

Wieder sagen wir OpenGL das wir die Projektions-Matrix ändern wollen und laden dann eine Einheits-Matrix. Als nächstes berechnen wir den aspectRatio als Viewport Breite durch Viewport Höhe. Dieser Wert ist eine Dezimalzahl daher der Cast auf (float). Zu guter letzt verwenden wir wieder GLU und dessen Methode ''gluPerspective''. Der erste Parameter ist die GL Instanz, der zweite Parameter das Field of View in Grad, wobei 67 Grad ungefähr dem Sichtfeld nach oben und unten eines Menschen entsprechen. Der nächste Parameter gibt die Distanz zur near Clipping Plane an, hier setzen wir ihn auf 1. Der letzte Parameter gibt die Distanz zur far Clipping Plane an, hier 100, und wir sind auch schon fertig mit der Konfiguration der perspektivischen Projektion. 

Jetzt stellt sich uns die Frage wo in unserem 3D Koordinaten-System sich der Betrachter befindet. Wir erinnern uns, die positive x-Achse zeigt nach rechts, die positive y-Achse nach oben und die positive z-Achse aus dem Bildschirm heraus. Die negative z-Achse zeigt somit in den Bildschirm hinein. Der Betrachter befindet sich im Ursprung, also an den Koordinaten (0,0,0) und schaut gerade entlang der '''negativen''' z-Achse. Die near Clipping Plane befindet sich damit an der z-Koordinate -1, die far Clipping Plane an der z-Koordinate -101. Für unser Mesh bedeutet das, dass wir es auf z irgendwo zwischen -1 und -100 ansiedeln müssen. Hier ein Beispiel mit 2 Dreiecken, eines auf z=-2 und ein zweites auf z=-5. Beide Dreiecke haben die selbe Größe:

 mesh = new Mesh( gl, 6, true, false, false );				
 mesh.color( 0, 1, 0, 1 );
 mesh.vertex( 0f, -0.5f, -5 );
 mesh.color( 0, 1, 0, 1 );
 mesh.vertex( 1f, -0.5f, -5 );
 mesh.color( 0, 1, 0, 1 );
 mesh.vertex( 0.5f, 0.5f, -5 );		
 mesh.color( 1, 0, 0, 1 );
 mesh.vertex( -0.5f, -0.5f, -2 );
 mesh.color( 1, 0, 0, 1 );
 mesh.vertex( 0.5f, -0.5f, -2 );
 mesh.color( 1, 0, 0, 1 );
 mesh.vertex( 0, 0.5f, -2);

Das erste Dreieck ist ein wenig nach rechts verschoben und liegt hinter dem zweiten Dreieck. Die Dreiecke werden von OpenGL auch in dieser Reihenfolge gezeichnet, wodurch das hintere grüne Dreieck vom vorderen roten Dreieck überdeckt wird:

[[Datei:perspectivetri.png]]

Wie zu erwarten ist das grüne Dreieck kleiner als das rote da es ja auch weiter entfernt ist. Wir haben somit den Schritt in die 3te Dimension geschafft! Den Sample Code findet ihr unter [[http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/PerspectiveSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/PerspectiveSample.java]

=== Kamera, Z-Buffer und wie Lösch ich den Schirm ===
Eine Kamera die man nicht bewegen kann ist äußerst langweilig. Zum Glück ist es mit Hilfe der Klasse GLU extrem einfach das zu ändern. Dazu müssen wir uns aber zuerst vor Augen führen wie eine Kamera definiert ist. Zum einen hat sie natürlich eine Position in unserer 3D-Welt. Auch muss sie eine Richtung besitzen in die sie schaut. Aus dem Vektor Kapitel wissen wir wie wir das abbilden können. Ein Baustein fehlt noch: der sogenannte Up Vektor. Dieser definiert die y-Achse der Kamera, die Richtung definiert die z-Achse der Kamera und die x-Achse können wir leicht über das sogenannte Kreuz-Produkt aus Up und Richtungsvektor errechnen. Das brauchen wir aber alles gar nicht da uns das die GLU Klasse abnimmt. Zur Verständnis des Up-Vektors: man stelle sich vor ein Pfeil ragt einem senkrecht aus dem Kopf. Das ist der Up Vektor. Neigt man sein Haupt nun nach rechts oder links änder sich auch dieser Up Vektor und mit ihm der Winkel unter dem man das Bild sieht. Aus mathematischer Sicht sei noch angemerkt, dass dieser Up Vektor und der Richtungsvektor Einheitsvektoren sind und normal auf einander stehen, also im neunzig Grad Winkel. 

Damit wir unsere Welt aus der Sicht der Kamera sehen müssen wir wieder eine Matrize von OpenGL bemühen. Diese nennt sich die Model-View-Matrize. Der View Teil bezeichnet dabei den Umstand das man in dieser Matrize die Kamera-Matrize (die sich aus den oben genannten Eigenschaften der Kamera ergibt) ablegt. Ein Vertex wird zuerst durch die Model-View-Matrize transformiert (per Multiplikation) und dann mit der Projektions-Matrix multipliziert um seine finale Position zu bestimmen. Schaun wir uns also an wie wir diese Model-View-Matrix mit GLU so setzen können das wir unsere Welt aus der Sicht des Kamera sehen:

 gl.glMatrixMode( GL10.GL_MODELVIEW );
 gl.glLoadIdentity();
 GLU.glLookAt( gl, positionX, positionY, positionZ, zentrumX, zentrumY, zentrumZ, upX, upY, upZ );
 
Die ersten beiden Zeilen kennen wir ja schon. In der ersten sagen wir jedoch, dass wir die Model-View-Matrix verändern möchten. In der dritten Zeile definieren wir unsere Kamera aus der GLU dann eine Matrize errechnet und die Model-View-Matrix auf diese setzt. Der erste Parameter ist wieder die GL Instanz. Die nächsten drei Parameter geben die Koordinaten der Kamera an. zentrumX, zentrumY und zentrumZ geben einen Punkt in der Welt an auf den die Kamera blicken soll. Nimmt man diesen Punkt und subtrahiert man davon die Kamera Position vektoriel erhält man die Richtung der Kamera und damit deren z-Achse. Die letzten drei Parameter geben den oben beschriebenen Up Vektor an. Dieser muss ein Einheits-Vektor sein, also die Länge 1 besitzen sonst können komische Ergebnisse auftreten. 

Ziehen wir unsere Szene mit dem roten und dem grünen Dreieck aus dem letzten Beispiel heran. Wir wollen die Kamera jetzt hinter das grüne Dreieck positionieren (also z<-5) und sie in Richtung Ursprung sehen lassen. Die Neigung lassen wir dabei normal, d.h. der Up Vektor schaut nach oben (0,1,0):

  gl.glMatrixMode( GL10.GL_MODELVIEW );
  gl.glLoadIdentity();
  GLU.glLookAt( 0, 0, -7, 0, 0, 0, 0, 1, 0 );

Das Ergebnis sieht so aus ( Sample Code unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/CameraSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/CameraSample.java] )

[[Datei:camera.png]]

Da scheint etwas schief gegangen zu sein. Eigentlich dürfte das rote Dreieck ja das grüne nicht überdecken, tut es aber. Das Problem: Im Mesh haben wir das rote Dreieck nach dem grünen definiert und in der Reihenfolge werden sie auch gezeichnet. Betrachten wir das ganze von vorne stimmt alles. Von unten passt es aber nicht mehr. Was tun? In Abhängigkeit von der Blickrichtung die Ordnung der Dreiecke ändern? Was macht man dann bei sich überlappenden Dreiecken wo die Ordnung nicht eindeutig ist?

Für all diese Probleme gibt es eine Lösung mit dem klingenden Namen [[http://de.wikipedia.org/wiki/Z-Buffer Z-Buffer]. Dieser ist quasi ein Zusatz zum Frame Buffer (wir erinnern uns, dort werden alle Pixel gespeichert) und besitzt die selbe Größe. Jeder Pixel eines gezeichneten Dreiecks besitzt neben seiner x- und y-Koordinate nach der Transformation mit der Projektions- und Model-View-Matrix auch eine z-Koordinate. In den Z-Buffer schreibt OpenGL genau diese z-Koordinate und macht noch etwas besonders schlaues: bevor es überhaupt einen Pixel zeichnet prüft es ob im Z-Buffer bereits ein Pixel existiert der näher zur Kamera liegt. Ist dem der Fall braucht OpenGL den aktuellen Pixel nicht schreiben da er ja hinter dem aktuellen Pixel liegt. Alles was wir also tun müssen ist diesen Z-Buffer einzuschalten und das geht so:

 gl.glEnable(GL10.GL_DEPTH_TEST);

Dazu müssen wir aber noch etwas tun und zwar den Z-Buffer in jedem Frame löschen. Und wenn wir schon dabei sind dann können wir auch gleich den Frame Buffer mitlöschen, was so geht:

 gl.glClear(GL10.GL_COLOR_BUFFER_BIT | GL10.GL_DEPTH_BUFFER_BIT);
 
Wer auch noch die Farbe mit der der Frame Buffer gelöscht werden soll bestimmen will kann dies folgendermaßen tun:

 gl.glClearColor( red, green, blue, alpha );
  
Der Frame und der Z-Buffer sollte möglichst in jedem Frame gelöscht werden. Normalerweise mache ich das immer ganz am Anfang des Rendering damit ich nicht darauf vergesse. Schaun wir uns an wie unser Dreieck-Sortierproblem jetzt ausschaut:

[[Datei:zbuffer.png]]
  
Ausgezeichnet, und all das mit nur zwei zusätzlichen Befehlen. Mit dem Z-Buffer kommen aber auch Probleme:

* Wenn zwei Dreiecke in der selben Ebene liegen und Überlappen kommt es zum sogenannten [http://en.wikipedia.org/wiki/Z-fighting Z-Fighting]. Dies tritt vor allem auf wenn man beim Rendern mit orthographischer Projektion vergisst den Z-Buffer mit ''glDisable(GL10.GL_DEPTH_TEST)'' auszuschalten. Wir werden immer daran denken dies vor dem Zeichnen von 2D Elementen zu tun!
* Das Problem der Sortierung wird bei transparenten Dreicken, also solchen durch die der Hintergrund etwas zu sehen ist, nicht gelöst. Man stelle sich ein Dreieck vor welches hinter einem anderen in der Szene liegt. Das verdeckende Dreieck ist transparent und wird vor dem hinteren Dreieck gerendert. Ergebnis: das hintere Dreieck kann nicht durchscheinen da seine Pixel gar nicht erst in den Frame Buffer geschrieben werden. Im Z-Buffer befinden sich ja schon die Werte für das vordere Dreieck welche näher an der Kamera sind. Dieses Problem löst man im allgemeinen in dem man zuerst alle nicht transparenten Objekte zeichnet, dann alle transparenten Objekte über die Distanz zur Kamera sortiert und in der sortierten Reihenfolge rendert.

Beachtet man diese beiden Probleme steht dem vergnüglichen Gebrauch des Z-Buffers nichts im Weg!

=== Licht und Schatten ===
3D allein macht noch kein 3D Gefühl. Das menschliche Auge verwendet nicht nur das stereoskopische Sehen zum Abschätzen von Tiefe sondern auch andere Hinweise und hier vor allem Licht & Schatten.

OpenGL bietet hier einiges an Möglichkeiten zumindest was Licht betrifft. Schattenwurf wie wir in kennen ist nicht direkt in OpenGL inkludiert kann aber nachgebaut werden. Mit OpenGL ES ist dies jedoch zu rechenaufwendig wir werden uns daher nur um das Licht kümmern. Schatten bekommen wir in der Sparversion: vom Licht abgewendete Seiten sind dunkler.

Um OpenGL dazu zu bewegen Licht zu simulieren müssen wir dieses einfach anknipsen:

 gl.glEnable(GL10.GL_LIGHTING);

Das ausschalten funktioniert analog:

 gl.glDisable(GL10.GL_LIGHTING);

Für unsere 2D Elemente werden wir kein Licht brauchen sondern wollen deren vollen Farben haben. Darum müssen wir vor dem Zeichnen der 2D Elemente das Licht auch ausschalten.

OpenGL kann verschiedene Lichtarten und Lichtquellen simulieren. Wo liegt hier der Unterschied? Als Lichtarten gelten ambientes Licht, diffuses Licht, spekulares Licht und emissives Licht (streng genommen keine Lichtart). Ambientes Licht kommt aus allen Richtungen und hat keine bestimmte Quelle. Es handelt sich um jene Photonen die schon tausende Male von einem Objekt reflektiert wurden und so für einen "Grundlichtpegel" sorgen. Diffuses Licht geht von einer bestimmten Lichtquelle aus und wird aufgrund von feinen unebenheiten des das Licht reflektierenden Objekts in alle möglichen Richtungen reflektiert. Spekulares Licht hingegen wird scharf reflektiert, z.B. auf einem Spiegel und bildet an einem bestimmten Punkt am Objekt ein sogenanntes Highlight, einen überbeleuchteten Punkt. Emissives Licht ist Licht das vom bestrahlten Objekt selbst ausgeht. Das folgende Bild zeigt alle vier Typen: ambient, diffuse, spekular und emissiv.

[[Datei:lighttypes.png]]

Wir werden uns nur mit ambienten und diffusen Licht in OpenGL beschäftigen. Spekulares Licht benötigt ein sehr fein aufgelöstest Mesh und damit viele Dreiecke um richtig zur Geltung zu kommen. Emissives Licht können wir auch über die Farbe des Meshes simulieren, was in der Regel einfacher ist.

Als Lichtquellen gelten Punktlichtquellen, wie etwa eine Lampe, deren Strahlen radial ausstrahlen, direktionale Lichtquellen, wie etwa die Sonne deren  Strahlen aufgrund der Entfernung alle so gut wie parallel bei uns auftreffen und Spotlichtquellen, wie ein gerichteter Scheinwerfer der einen Lichtkegel bildet. Punktlichtquellen und Spotlichtquellen besitzen eine Position im Raum. Eine direktionale Lichtquelle wird in OpenGL als unendlich weit entfernt angenommen und hat deswegen nur eine Richtung. Wir werden uns nur mit Punktlichtquellen und direktionalen Lichtquellen beschäftigen. Für Spotlichtquellen gilt wieder ähnliches wie für spekulares Licht, sie benötigen hoch aufgelöste Meshes um zur Geltung zu kommen. Jeder Lichttyp emitiert ambientes, diffuses und spekulares Licht. Wenn wir eine Lichtquelle definieren müssen wir für jeden Lichttypen die Farbe angeben die die Lichtquelle für diesen Typen emitiert. OpenGL ES kann insgesamt 8 Lichtquellen zugleich simulieren. Diese Lichtquellen werden von 0 bis 7 durchnummeriert und werden mit den Konstanten GL10.GL_LIGHT0 bis GL10.GL_LIGHT7 identifiziert. Schauen wir uns zuerst an wie man die Farben der Lichttypen eines Lichtes definiert:

 float lightColor[] = {1, 1, 1, 1.0};
 float ambientLightColor[] = { 0.2f, 0.2f, 0.2f, 1.0 };
 gl.glLightfv(GL.GL_LIGHT0, GL10.GL_AMBIENT, lightColor, 0 );
 gl.glLightfv(GL.GL_LIGHT0, GL10.GL_DIFFUSE, lightColor, 0 );
 gl.glLightfv(GL.GL_LIGHT0, GL10.GL_SPECULAR, lightColor, 0 );

In der ersten Zeile basteln wir einen Array mit der Lichtfarbe, schönes weiss. Für die ambiente Komponente definieren wir ein dunkles Grau in Zeile 2. Zeilen 3 bis 5 setzt dann die Farbe für jeden Lichttypen der Lichtquelle 0. So einfach geht das. Natürlich kann man für jeden Lichttyp eine verschiedene Farbe angeben, hier kann man experimentieren. 

Ob die Lichtquelle ein direktionales Licht oder ein Punktlicht ist definiert man ebenfalls über die Methode ''glLightfv''. Anstatt als zweiten Parameter den Lichttyp anzugeben (z.B: GL10.GL_AMBIENT) verwendet man aber die Konstante GL10.GL_POSITION. Auch übergeben wir wieder einen float Array mit 4 Elementen. Ist das letzte Element gleich 0 so sagt dies OpenGL dass wir ein direktionales Licht haben wollen. Die drei ersten Elemente geben dann die '''negative''' Richtung des Lichts an. Diese Richtung muss ein Einheitsvektor sein! Man kann sich dies auch als die Position einer Lichtquelle gelten. Die Richtung ist dann der Vektor von der Lichtquelle zum Ursprung. Sehr verwirrent. Ist das vierte Element gleich 1 so sagen wir OpenGL damit das wir ein Punktlicht wollen. Die ersten drei Elemente im Array entsprechen dann der Position des Lichts in der Welt.

Ein direktionales Licht von Links würde dem zur folge so definiert:

 float[] direction = { 1, 0, 0, 0 };
 gl.glLightfv(GL.GL_LIGHT0, GL10.GL_POSITION, direction, 0 );

Eine Punktlichtquelle direkt über dem Ursprung gibt man so an:

 float[] position = { 0, 10, 0, 1 };
 gl.glLightfv(GL.GL_LIGHT0, GL10.GL_POSITION, position, 0 );

Wie Licht von einem Objekt reflektiert wird hängt nicht nur vom Lichttyp und der Lichtquelle ab. Auch das Material des Objektes spielt dabei eine Rolle. OpenGL ES hat einen relativ elaborierten Mechanismus um das Material eines Objekts zu definieren. So elaboriert dieser ist so langsam ist er leider auch. Wieder müssen wir für jeden Lichttyp eine Farbe definieren. Dies würden wir mit der Methode glMaterialfv machen. Diese ist aber wie gesagt extrem langsam. Wir nehmen hier eine Abkürzung und verwenden eine spezielle Methode von OpenGL ES:

 gl.glEnable(GL_COLOR_MATERIAL);

Dies weißt OpenGL ES an das es anstatt eines definierten Materials einfach die Farbe des Vertex hernehmen soll und dieses für die ambiente und diffuse Komponente verwenden soll. In der Regel kommt man damit vor allem auf kleinen Bildschirmen locker durch. 

Wer sich noch an das Kapitel Mesh & Texture Klasse erinnern kann denkt vielleicht jetzt an die Normalen die wir pro Vertex gleich wie Farbe oder Texture-Koordinaten angeben kann. Diese brauchen wir auch '''unbedingt''' wenn wir OpenGLs Beleuchtungsmodel verwenden wollen. Was ist also so eine Vertex-Normale? Ein kleines Bild:

[[Datei:normals.jpg]]

An jeder Ecke des Würfels sitzen 3 Vertices. Wenn wir darüber nachdenken wird schnell klar warum: jede Seite die auf diese Ecke trifft hat ein Dreieck an dieser Stelle. Da wir es nicht besser wissen würden wir für den Würfel insgesamt 6 * 2 = 12 Dreiecke haben und damit 6 * 2 * 3 = 36 Vertices. Jeder Vertex hat nun eine Normale. Diese Normale ist normal zur Ebene in der das Dreieck liegt und ragt aus der Vorderseite des Dreiecks. OpenGL verwendet diese Normale um den Winkel des Vertices zur Lichtquelle zu berechnen. Darum müssen wir die in Meshes die wir beleuchten wollen auch unbedingt angeben. Im Source zur Mesh Klasse könnt ihr euch anschaun wie man die Normale eines Vertex OpenGL übergibt. Der Mechanismus ist 1:1 der selbe wie bei Farben und Texture-Koordinaten, darum gehe ich hier nicht gesondert nochmal drauf ein. Zur Übung versuchen wir einfach schnell ein Mesh zu machen das den drei in der obigen Zeichnung eingezeichneten Dreiecken entspricht. Wir werden es so definieren das das dunkelste Dreieck in der x-y-Ebene liegt:

 mesh = new Mesh( gl, 9, false, false, true );
 mesh.normal( 0, 0, 1 );
 mesh.vertex( 1, 0, 0 );
 mesh.normal( 0, 0, 1 );
 mesh.vertex( 1, 1, 0 );
 mesh.normal( 0, 0, 1 );
 mesh.vertex( 0, 1, 0 );
 mesh.normal( 1, 0, 0 );
 mesh.vertex( 1, 0, 0 );
 mesh.normal( 1, 0, 0 );
 mesh.vertex( 1, 0, -1);
 mesh.normal( 1, 0, 0 );
 mesh.vertex( 1, 1, 0 );
 mesh.normal( 0, 1, 0 );
 mesh.vertex( 1, 1, 0 );
 mesh.normal( 0, 1, 0 );
 mesh.vertex( 1, 1, -1 );
 mesh.normal( 0, 1, 0 );
 mesh.vertex( 0, 1, 0 );

Pfuh, ganz schön viel Code für 3 Dreiecke. Wir werden da später abhilfe schaffen. Beim Rendern müssen wir jetzt ein paar Dinge erledigen. Zum einen die Beleuchtung einschalten. Danach die Lichtquelle definieren. Dann die Lichtquelle einschalten und vor dem Rendern des Mesh noch Color Material enablen. Als Lichtquelle nehmen wir eine weisses direktionale die von Rechts oben kommt (-1, -1, 0): 

 gl.glEnable( GL10.GL_LIGHTING );
 float[] lightColor = { 1, 1, 1, 1 };
 float[] ambientLightColor = {0.2f, 0.2f, 0.2f, 1 };
 gl.glLightfv( GL10.GL_LIGHT0, GL10.GL_AMBIENT, ambientLightColor );
 gl.glLightfv( GL10.GL_LIGHT0, GL10.GL_DIFFUSE, lightColor );
 gl.glLightfv( GL10.GL_LIGHT0, GL10.GL_SPECULAR, lightColor );
 float[] direction = { -1 / (float)Math.sqrt(2), -1 / (float)Math.sqrt(2), 0, 0 };
 gl.glLightfv( GL10.GL_LIGHT0, GL10.GL_POSITION, direction );
 gl.glEnable( GL10.GL_LIGHT0 );
 gl.glEnable( GL10.GL_COLOR_MATERIAL );
 mesh.render(PrimitiveType.Triangles);

Wieder ein Höllen Aufwand. Wir haben auch ein paar Faux Pas geschossen. Zum einen würden wir so im Main Loop permanent zwei neue float Arrays instanzieren. Das würde irgendwann den Garbage Collector verstimmen der sich dann ein paar hundert Millisekunden Auszeit nimmt um auf zu räumen. Wir werden im Sample Code die beiden Arrays zu Klassen Member unserer Sample Activity machen und somit nur einmal instanzieren. Zweitens müssen wir eine Lichtquelle nicht immer neu definieren. So sich diese über den Verlauf nicht ändert reicht es deren Lichttypen Farben nur einmal anzugeben. Die Position/Richtung der Lichtquelle müssen wir aber '''immer''' nach dem Aufruf von ''GLU.glLookAt'' machen, da die Position sonst in einem anderen Koordinaten-System definiert wird. Verwirrent. Wer sich übrigens fragt warum wir die direction Elemente durch Math.sqrt(2) dividieren: Hier normalisieren wir den Richtungsvektor! ( (-1, -1, 0)' = [-1 / |(-1, -1, 0)|, -1 / |(-1, -1, 0), 0 / |(-1, -1, 0)] ).

Um die Situation im Bild oben nachzustellen werden wir auch die Kamera Position und Richtung entsprechend setzen. Den genauen Code könnt ihr hier sehen [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/LightSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/LightSample.java]. Das Ergebnis sieht so aus:

[[Datei:lightsample.png|320px]]

Eine Punktlichtquelle würde komplett analog dazu definiert und eingesetzt werden, mit dem Unterschied im 4ten Element im Positions Array. Und wieder ein Geheimnis von OpenGL gelüftet!

=== Transformationen ===
Jetzt wird's noch mal kurz mathematisch. Vielleicht hat sich der eine oder andere bereits gefragt wie man den ein Mesh an verschiedenen Positionen mehrere Male zeichnen kann. Schließlich sieht man das ja auch in anderen Spielen, z.B. in einem Echtzeitstrategiespiel wo der selbe Einheitentyp mehrere male gezeichnet wird nur an verschiedenen Positionen und unterschiedlicher Ausrichtung. In OpenGL verwendet man dazu wieder Matrizen, genauer, die bereits erwähnte Model-View-Matrize. Hier erklärt sich auch der erste Teil des Namens der Matrize: Model steht für die möglichkeit ein Model (Mesh) in der Welt zu verschieben, zu rotieren und zu skalieren (größer und kleiner machen). 

Unter einer Transformation versteht man die Verschiebung (Translation), Skalierung und Rotation von Vertices in der Welt. Dabei kann man mehrere solcher Transformationen über die Model-View-Matrix kombinieren, z.B. zuerst skalieren, dann rotieren und zum Schluss verschieben. Mathematisch gesehen entspricht das der Multiplikation von Matrizen: für jede Transformation wird eine Matrix erstellt, diese werden dann in der gewünschten Reihenfolge der Transformationen miteinander multipliziert. Wieder müssen wir uns zum Glück nicht direkt mit Matrizen herumschlagen, OpenGL bietet uns verschiedene Methoden die das erstellen und multiplizieren der Matrizen für uns erledigt. 

Fangen wir mit der Translation an. Diese wird über einen Translations-Vektor angegeben der zu allen Vertices die man rendert hinzuaddiert wird:

[[Datei:translation.png]]

In OpenGL ES erreichen wir das, indem wir folgende Methode verwenden:

 gl.glTranslatef( x, y, z );

Wie unschwer zu erkennen handelt es sich bei den 3 Parametern um den Translations-Vektor. Diese Methode erstellt intern eine Translations Matrix und multipliziert die aktuel aktive Matrix damit, z.B. die Model-View-Matrix die wir über ''glMatrixMode'' ausgewählt haben. 

Die Skalierung multipliziert jede Komponente der Vertex Position mit einem Skalierungsfaktor:

[[Datei:scaling.png]]

OpenGL stellt dafür folgende Methode zur Verfügung:

 gl.glScalef( scaleX, scaleY, scaleZ );

Für jede der drei Achsen gibt es einen eigenen Skalierungsfaktor. Wieder wird intern eine Matrix erstellt, mit den Werten für die Skalierung befüllt und dann mit der aktuel aktiven Matrix multipliziert. 

Die Rotation ist ein wenig schwieriger zu verstehen. Gedreht wird immer um den Ursprung. Gleichzeitig müssen wir eine Achse angeben (die implizit durch den Ursprung geht) um die sich die Vertices drehen sollen. 

[[Datei:rotation.png]]

Die OpenGL Methode dafür:

 gl.glRotatef( angle, axisX, axisY, axisZ );

Angle gibt den Winkel in Grad an, axisX bis axisZ ist die Rotationsachse um die gedreht werden soll. Im obigen Beispiel ist diese (0, 0, 1). Wie bei vielen anderen Dingen muss die Rotationsachse ein Einheitsvektor sein. 

Diese drei Transformationen können wir beliebig miteinander kombinieren, z.B. verschieben, rotieren, verschieben, skalieren usw. Als aktive Matrix wählen wir für diese Transformationen immer die Model-View-Matrix über ''glMatrixMode''. Schaun wir uns einmal an was die Kombination von Translation und Rotation bewirkt:

[[Datei:transform1.png]]

Zuerst verschieben wir das Dreieck ein wenig nach rechts, dann rotieren wir um die positive z-Achse. Wenn wir das ganze umdrehen schaut das ergebnis so aus:

[[Datei:transform2.png]]

Ein komplett anderes Ergebnis. Wir müssen bei der Anwedung von Transformationen immer auf die Reihenfolge schaun. Die erste Beispiel würden wir mit OpenGL so realisieren:

 gl.glRotatef( 45, 0, 0, 1 );
 gl.glTranslatef( 2, 0, 0 );

Hm, sollte das nicht umgekehrt sein? Wir wollen ja zuerst verschieben und dann rotieren. OpenGL ist da anderer Ansicht, die letzte Transformation die wir über die Transformations Methoden angeben ist immer die erste die auf die Vertices wirkt. Dies resultiert aus der Art wie OpenGL Matrizen multipliziert und soll uns hier nicht weiter kümmern. Wir müssen uns nur den Umstand merken, dass wir Transformationen immer in der umgekehrten Reihenfolge ausführen müssen. 

Was ich bis jetzt verschiegen habe ist der Zusammenhang zwischen Transformationen und der Kamera. Wir befüllen in 3D ja die Model-View-Matrix per ''GLU.gluLookAt'' bereits mit der Kamera Matrix. Zeichnen wir nun mehrere Objekte mit jeweils eigenen Transformationen müssten wir die Kamera Matrix vor dem Zeichnen eines Objektes jedesmal neue setzen da wir ja die Model-View-Matrix beim vorhergehenden Objekt überschrieben haben. Um diese recht kostspielige Operation zu vermeiden gibt es zwei Befehle:

 gl.glPushMatrix();
 gl.glPopMatrix();

In Wirklichkeit gibt es für jede Matrix in OpenGL (Projektion, Model-View) nicht nur eine Matrix sondern einen Stack an Matrizen. Mit den oben vorgestellten Methoden manipulieren wir immer den Top of Stack. Die beiden Methoden ''glPushMatrix()'' und ''glPopMatrix()'' erlauben es uns die über ''glMatrixMode'' aktuel selektierte Matrix auf den Stack zu legen, bzw. die zuletzt auf den Stack gelegte Matrix wieder zur aktuellen Matrix zu machen. Beim pushen der Matrix wird eine Kopie angelegt, die aktuelle Matrix bleibt die selbe.

In Spielen geht man in der Regel so vor: jedes Objekt in der Spielwelt hat eine Orientierung (also eine Richtung in die es schaut) und eine Position. Die Meshes für die Objekte sind immer um den Ursprung definiert. Zeichnet man nun die Objekte läd man zu aller erst die Kamera-Matrix in die Model-View-Matrix. Beim zeichnen jedes Objektes pushen wir die Model-View-Matrix damit wir eine Kopie der Kamera-Matrix am Stack haben, multiplizieren dann die Transformationen auf die Model-View-Matrix, zeichnen das Mesh des Objekts, welches somit richtig transformiert wird und popen die Model-View-Matrix wieder vom Stack womit diese wieder nur die Kamera-Matrix beinhaltet. Diesen Prozess wiederholen wir für alle Objekte die wir zeichnen. So werden wir das dann auch in unserem Space Invaders Clone machen.

Damit haben wir den letzten großen Brocken was OpenGL betrifft abgearbeitet. Wie für Licht gilt: experimentieren, experimentieren, experimentieren. Um Transformationen zu verstehen muß man sie in Aktion sehen. Als kleine Aufgabe könnt ihr ja eines der Samples hernehmen und ein um die y-Achse rotierendes Dreieck produzieren. Dazu müsst ihr nur in jedem Frame ''glRotatef'' mit dem aktuellen Winkel aufrufen. Den Winkel muss man natürlich in jedem Frame erhöhen und wenn er größer als 360 ist wieder auf 0 zurücksetzen.

=== Meshes laden ===
Nachdem wir den Schritt in die dritte Dimension gewagt haben wäre es natürlich nicht schlecht eine Möglichkeit zu besitzen Meshes aus anderen Programmen zu laden. Eines der einfachsten Mesh Formate ist das [http://en.wikipedia.org/wiki/Obj Wavefront OBJ Format]. Ich habe mir die Freiheit genommen dafür einen einfachen Loader zu schreiben. Dieser kann mit Obj-Dateien umgehen die nur Dreiecke beinhalten. Baut ihr also eure eigenen Meshes in [http://www.wings3d.com/ Wings3D] oder [http://www.blender.org/ Blender] könnt ihr eure Meshes von dort aus in eine Obj-Datei exportieren und mit dem Loader laden. Der Loader besitzt nur eine statische Methode:

 Mesh MeshLoader.loadObj( GL10 gl, InputStream in )
 
Zum Laden übergeben wir also nur einen InputStream auf ein Obj-Asset und erhalten ein fix und fertiges Mesh zurück. So das Mesh Normalen oder Texture-Koordinaten hat werden diese natürlich mitgeladen und können dann mit einer Lichtquelle bzw. Texture verwendet werden. Ich habe in Wings3D ein kleines Raumschiff gebaut und mit Gimp dafür eine Texture erstellt. Das Obj-File und die Texture verwende ich in einem weiteren Sample das ihr unter [http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/ObjSample.java http://code.google.com/p/android-gamedev/source/browse/trunk/src/com/badlogic/gamedev/samples/ObjSample.java] anschaun könnt. Es ist im Grunde das Light Sample mit dem Unterschied dass ich eine Texture und das Mesh aus dem Obj-File lade. Außerdem habe ich mir erlaubt hier die Aufgabe aus dem Transformations Kapitel umzusetzen. Das Schiff dreht sich hübsch. Hier noch ein Screenshot

[[Datei:ship.png]]

Jetzt haben wir aber wirklich alles besprochen was es zu besprechen gibt. Auf zum Sound!  

== SoundPool und MediaPlayer ==
= Space Invaders =

[[Kategorie:Entwicklung]]
= Einleitung =

